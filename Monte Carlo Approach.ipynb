{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5059f410-2f6c-44ba-b751-465588772b56",
   "metadata": {},
   "source": [
    "Your calculate_var function looks well-structured for calculating Value at Risk (VaR) using Monte Carlo simulations based on a Poisson distribution for event counts and a log-normal distribution for severity amounts. Here‚Äôs a detailed breakdown of your function and some suggestions for improvement or clarity:\n",
    "\n",
    "Breakdown of the Function\n",
    "Parameters:\n",
    "\n",
    "frequency_lambda: The mean rate (Œª) of the Poisson distribution, representing the average number of events in a given period.\n",
    "severity_shape: The shape parameter (s) for the log-normal distribution, which characterizes the distribution's skewness.\n",
    "severity_scale: The scale parameter for the log-normal distribution, influencing the distribution's spread.\n",
    "severity_loc: The location parameter for the log-normal distribution, which shifts the distribution.\n",
    "confidence_level: The desired confidence level for VaR (default is set to 0.999).\n",
    "n_simulations: The number of Monte Carlo simulations to run (default is 10,000).\n",
    "Simulations:\n",
    "\n",
    "The function uses the Poisson distribution to simulate the number of events occurring (simulated_event_counts).\n",
    "It also simulates loss amounts using a log-normal distribution (simulated_loss_amounts).\n",
    "The total loss for each simulation is computed by multiplying the number of events by the severity of each event.\n",
    "VaR Calculation:\n",
    "\n",
    "Finally, the function calculates the VaR by finding the specified percentile of the total losses, which indicates the maximum expected loss under normal market conditions over a defined time horizon at the given confidence level.\n",
    "Suggestions for Improvement\n",
    "Import Statements: Make sure to include the necessary import statements at the beginning of your script if you haven't already:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from scipy.stats import poisson, lognorm\n",
    "Input Validation: Consider adding input validation to ensure parameters like frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level, and n_simulations are within acceptable ranges.\n",
    "\n",
    "Documentation: Adding a docstring to the function can improve readability and usability. Here‚Äôs an example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "\"\"\"\n",
    "Calculate the Value at Risk (VaR) using Monte Carlo simulation.\n",
    "\n",
    "Parameters:\n",
    "- frequency_lambda (float): The mean rate of events (Œª) for Poisson distribution.\n",
    "- severity_shape (float): The shape parameter for log-normal distribution.\n",
    "- severity_scale (float): The scale parameter for log-normal distribution.\n",
    "- severity_loc (float): The location parameter for log-normal distribution.\n",
    "- confidence_level (float): Confidence level for VaR (default is 0.999).\n",
    "- n_simulations (int): Number of Monte Carlo simulations (default is 10000).\n",
    "\n",
    "Returns:\n",
    "- float: The calculated VaR at the specified confidence level.\n",
    "\"\"\"\n",
    "Return Additional Information: If needed, you might want to return additional statistics, like the mean loss or the standard deviation of the simulated losses for more insights.\n",
    "\n",
    "Performance Considerations: For large-scale simulations, you may explore vectorized operations to speed up calculations or use libraries like Numba for just-in-time compilation if performance is a concern.\n",
    "\n",
    "Final Code Example\n",
    "Here‚Äôs your function with the suggested improvements:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from scipy.stats import poisson, lognorm\n",
    "\n",
    "def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):\n",
    "    \"\"\"\n",
    "    Calculate the Value at Risk (VaR) using Monte Carlo simulation.\n",
    "\n",
    "    Parameters:\n",
    "    - frequency_lambda (float): The mean rate of events (Œª) for Poisson distribution.\n",
    "    - severity_shape (float): The shape parameter for log-normal distribution.\n",
    "    - severity_scale (float): The scale parameter for log-normal distribution.\n",
    "    - severity_loc (float): The location parameter for log-normal distribution.\n",
    "    - confidence_level (float): Confidence level for VaR (default is 0.999).\n",
    "    - n_simulations (int): Number of Monte Carlo simulations (default is 10000).\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated VaR at the specified confidence level.\n",
    "    \"\"\"\n",
    "    # Simulate event counts based on Poisson\n",
    "    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)\n",
    "    # Simulate severity amounts based on Log-Normal\n",
    "    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)\n",
    "    \n",
    "    # Calculate total loss for each simulation\n",
    "    total_losses = simulated_event_counts * simulated_loss_amounts\n",
    "    \n",
    "    # Calculate the VaR at the specified confidence level\n",
    "    var_value = np.percentile(total_losses, confidence_level * 100)\n",
    "    \n",
    "    return var_value\n",
    "This function should serve you well for calculating VaR in various financial applications! If you have any specific use cases or further modifications in mind, feel free to ask!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15e7a9bb-e617-4d10-a78b-b6d9f974c06e",
   "metadata": {},
   "source": [
    "To implement a Monte Carlo simulation method using a Poisson distribution for event counts and a log-normal distribution for severity amounts, we need to follow these steps:\n",
    "\n",
    "Simulate Event Counts: Use the Poisson distribution to simulate the number of events (loss occurrences) based on an average rate (lambda).\n",
    "Simulate Severity Amounts: Use the log-normal distribution to simulate the severity of losses for each event.\n",
    "Calculate Total Losses: For each simulation iteration, calculate the total loss by summing the simulated severity amounts.\n",
    "Calculate VaR: After running multiple iterations, calculate the VaR from the distribution of total losses.\n",
    "Here's how this can be implemented in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d85e04-4688-469b-a78f-fd423fbfed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at Risk (99.9%): nan\n",
      "      Simulation Number  Total Loss Amount\n",
      "0                     1                inf\n",
      "1                     2                inf\n",
      "2                     3                inf\n",
      "3                     4                inf\n",
      "4                     5                inf\n",
      "...                 ...                ...\n",
      "9995               9996                inf\n",
      "9996               9997                inf\n",
      "9997               9998                inf\n",
      "9998               9999                inf\n",
      "9999              10000                inf\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Parameters for the simulation\n",
    "num_simulations = 10000  # Number of Monte Carlo simulations\n",
    "event_lambda = 5  # Average number of events (rate for Poisson distribution)\n",
    "mean_severity = 10000  # Mean severity for log-normal distribution\n",
    "stddev_severity = 5000  # Standard deviation for log-normal distribution\n",
    "\n",
    "# Function to perform Monte Carlo simulation\n",
    "def monte_carlo_simulation(num_simulations, event_lambda, mean_severity, stddev_severity):\n",
    "    total_losses = []\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Simulate the number of events\n",
    "        num_events = np.random.poisson(event_lambda)\n",
    "\n",
    "        # Simulate the severity amounts using log-normal distribution\n",
    "        if num_events > 0:\n",
    "            severity = np.random.lognormal(mean=np.log(mean_severity), sigma=stddev_severity, size=num_events)\n",
    "            total_loss = np.sum(severity)\n",
    "        else:\n",
    "            total_loss = 0  # No events means no loss\n",
    "\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "    return total_losses\n",
    "\n",
    "# Run the Monte Carlo simulation\n",
    "simulated_losses = monte_carlo_simulation(num_simulations, event_lambda, mean_severity, stddev_severity)\n",
    "\n",
    "# Calculate VaR at 99.9% confidence level\n",
    "var_99_9 = np.percentile(simulated_losses, 99.9)\n",
    "\n",
    "# Prepare results in a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Simulation Number': np.arange(1, num_simulations + 1),\n",
    "    'Total Loss Amount': simulated_losses\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(f\"Value at Risk (99.9%): {var_99_9}\")\n",
    "print(results)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "results.to_csv('monte_carlo_simulation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7085c3-76fe-4938-b526-0570ab5e5853",
   "metadata": {},
   "source": [
    "Explanation of the Code:\n",
    "Parameters:\n",
    "\n",
    "num_simulations: Number of times to run the Monte Carlo simulation.\n",
    "event_lambda: The average number of events (loss occurrences) modeled by the Poisson distribution.\n",
    "mean_severity: The mean of the log-normal distribution representing the severity of losses.\n",
    "stddev_severity: The standard deviation of the log-normal distribution.\n",
    "Monte Carlo Simulation Function:\n",
    "\n",
    "The function monte_carlo_simulation simulates the total loss for each iteration.\n",
    "It uses np.random.poisson to determine how many events occur and np.random.lognormal to determine the severity of those events.\n",
    "Calculate VaR:\n",
    "\n",
    "After running the simulation, it calculates the 99.9% VaR from the simulated total losses using np.percentile.\n",
    "Output:\n",
    "\n",
    "The total losses from all simulations are saved in a DataFrame for further analysis or storage.\n",
    "Conclusion:\n",
    "This approach uses a Monte Carlo simulation with a Poisson distribution for event counts and a log-normal distribution for loss severity, effectively capturing the dynamics of risk associated with operational losses. Adjust the parameters according to your specific scenario and requirements. If you have further requirements or questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746faaf1-48ed-434a-a9d9-2668f702d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at Risk (99.9%): nan\n",
      "      Simulation Number  Total Loss Amount\n",
      "0                     1                inf\n",
      "1                     2                inf\n",
      "2                     3                inf\n",
      "3                     4                inf\n",
      "4                     5                inf\n",
      "...                 ...                ...\n",
      "9995               9996                inf\n",
      "9996               9997                inf\n",
      "9997               9998                inf\n",
      "9998               9999                inf\n",
      "9999              10000                inf\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Function to perform Monte Carlo simulation for VaR calculation\n",
    "def monte_carlo_simulation(df, num_simulations=10000):\n",
    "    total_losses = []\n",
    "\n",
    "    # Estimate parameters for Poisson distribution (event counts)\n",
    "    event_lambda = df.shape[0] / df['Year'].nunique()  # Average number of events per year\n",
    "\n",
    "    # Estimate parameters for log-normal distribution (severity amounts)\n",
    "    mean_severity = abs(df['Net Loss Amount']).mean()  # Mean severity\n",
    "    stddev_severity = df['Net Loss Amount'].std()  # Standard deviation of severity\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Simulate number of events using Poisson distribution\n",
    "        num_events = np.random.poisson(event_lambda)\n",
    "\n",
    "        # Simulate severity amounts using log-normal distribution\n",
    "        if num_events > 0:\n",
    "            severity = np.random.lognormal(mean=np.log(mean_severity), sigma=stddev_severity, size=num_events)\n",
    "            total_loss = np.sum(severity)\n",
    "        else:\n",
    "            total_loss = 0  # No events means no loss\n",
    "\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "    return total_losses\n",
    "\n",
    "# Run the Monte Carlo simulation\n",
    "simulated_losses = monte_carlo_simulation(df, num_simulations=10000)\n",
    "\n",
    "# Calculate VaR at 99.9% confidence level\n",
    "var_99_9 = np.percentile(simulated_losses, 99.9)\n",
    "\n",
    "# Prepare results in a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Simulation Number': np.arange(1, len(simulated_losses) + 1),\n",
    "    'Total Loss Amount': simulated_losses\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(f\"Value at Risk (99.9%): {var_99_9}\")\n",
    "print(results)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "results.to_csv('monte_carlo_simulation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "625c1f6a-e085-48d4-bf74-e3ba10839c6a",
   "metadata": {},
   "source": [
    "Explanation of the Code:\n",
    "Data Generation: A dataset is created with random dates, event types, business lines, and net loss amounts.\n",
    "Monte Carlo Simulation:\n",
    "Event Count Simulation: The number of events is simulated using a Poisson distribution based on the average event rate.\n",
    "Severity Simulation: The severity of losses is modeled with a log-normal distribution using the mean and standard deviation of the absolute values of the net loss amounts.\n",
    "VaR Calculation: The 99.9% VaR is calculated from the simulated total losses.\n",
    "Output:\n",
    "The final output will show the calculated VaR at the 99.9% confidence level based on the Monte Carlo simulation, and the results will be saved to a CSV file for further analysis.\n",
    "\n",
    "Feel free to run this code and let me know if you have any further questions or modifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737b6920-f954-47a4-a3ab-3ad375b05693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at Risk (99.9%) for Loss: nan\n",
      "Value at Risk (99.9%) for Counts: 2143.0\n",
      "VaR Percentage Distribution for Loss: nan%\n",
      "VaR Percentage Distribution for Counts: 0.010713442801088862%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Function to perform Monte Carlo simulation for VaR calculation\n",
    "def monte_carlo_simulation(df, num_simulations=10000):\n",
    "    total_losses = []\n",
    "    event_counts = []\n",
    "\n",
    "    # Estimate parameters for Poisson distribution (event counts)\n",
    "    event_lambda = df.shape[0] / df['Year'].nunique()  # Average number of events per year\n",
    "\n",
    "    # Estimate parameters for log-normal distribution (severity amounts)\n",
    "    mean_severity = abs(df['Net Loss Amount']).mean()  # Mean severity\n",
    "    stddev_severity = df['Net Loss Amount'].std()  # Standard deviation of severity\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Simulate number of events using Poisson distribution\n",
    "        num_events = np.random.poisson(event_lambda)\n",
    "\n",
    "        # Simulate severity amounts using log-normal distribution\n",
    "        if num_events > 0:\n",
    "            severity = np.random.lognormal(mean=np.log(mean_severity), sigma=stddev_severity, size=num_events)\n",
    "            total_loss = np.sum(severity)\n",
    "        else:\n",
    "            total_loss = 0  # No events means no loss\n",
    "        \n",
    "        total_losses.append(total_loss)\n",
    "        event_counts.append(num_events)\n",
    "\n",
    "    return total_losses, event_counts\n",
    "\n",
    "# Run the Monte Carlo simulation\n",
    "simulated_losses, simulated_counts = monte_carlo_simulation(df, num_simulations=10000)\n",
    "\n",
    "# Calculate VaR at 99.9% confidence level\n",
    "var_99_9_loss = np.percentile(simulated_losses, 99.9)\n",
    "var_99_9_counts = np.percentile(simulated_counts, 99.9)\n",
    "\n",
    "# Total VaR for Losses and Counts\n",
    "total_var_loss = np.sum(simulated_losses)\n",
    "total_var_counts = np.sum(simulated_counts)\n",
    "\n",
    "# Calculate percentage distributions\n",
    "var_loss_percentage_distribution = (var_99_9_loss / total_var_loss) * 100 if total_var_loss != 0 else 0\n",
    "var_counts_percentage_distribution = (var_99_9_counts / total_var_counts) * 100 if total_var_counts != 0 else 0\n",
    "\n",
    "# Prepare results in a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Simulation Number': np.arange(1, len(simulated_losses) + 1),\n",
    "    'Total Loss Amount': simulated_losses,\n",
    "    'Event Counts': simulated_counts\n",
    "})\n",
    "\n",
    "# Display results\n",
    "print(f\"Value at Risk (99.9%) for Loss: {var_99_9_loss}\")\n",
    "print(f\"Value at Risk (99.9%) for Counts: {var_99_9_counts}\")\n",
    "print(f\"VaR Percentage Distribution for Loss: {var_loss_percentage_distribution}%\")\n",
    "print(f\"VaR Percentage Distribution for Counts: {var_counts_percentage_distribution}%\")\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "results.to_csv('monte_carlo_simulation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0449eb-827b-46c9-8359-b1f5205dbd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year      Business Line            Event Type  VaR (99.9%) for Loss  \\\n",
      "0    2020   Asset Management            Compliance                   NaN   \n",
      "1    2020   Asset Management          Cyber Attack                   NaN   \n",
      "2    2020   Asset Management                 Fraud                   NaN   \n",
      "3    2020   Asset Management           Market Risk                   NaN   \n",
      "4    2020   Asset Management      Natural Disaster                   NaN   \n",
      "..    ...                ...                   ...                   ...   \n",
      "494  2024  Wealth Management     Operational Error                   NaN   \n",
      "495  2024  Wealth Management  Regulatory Violation                   NaN   \n",
      "496  2024  Wealth Management        System Failure                   NaN   \n",
      "497  2024  Wealth Management                 Theft                   NaN   \n",
      "498  2024  Wealth Management           Vendor Risk                   NaN   \n",
      "\n",
      "     VaR (99.9%) for Counts  VaR Percentage Distribution for Loss  \\\n",
      "0                     8.000                                   NaN   \n",
      "1                    10.000                                   NaN   \n",
      "2                    11.000                                   NaN   \n",
      "3                     9.001                                   NaN   \n",
      "4                     9.000                                   NaN   \n",
      "..                      ...                                   ...   \n",
      "494                  46.000                                   NaN   \n",
      "495                  43.001                                   NaN   \n",
      "496                  43.000                                   NaN   \n",
      "497                  28.000                                   NaN   \n",
      "498                  30.000                                   NaN   \n",
      "\n",
      "     VaR Percentage Distribution for Counts  \n",
      "0                                  0.039944  \n",
      "1                                  0.032858  \n",
      "2                                  0.027310  \n",
      "3                                  0.029959  \n",
      "4                                  0.029858  \n",
      "..                                      ...  \n",
      "494                                0.016457  \n",
      "495                                0.016553  \n",
      "496                                0.016525  \n",
      "497                                0.018658  \n",
      "498                                0.018729  \n",
      "\n",
      "[499 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Function to perform Monte Carlo simulation for VaR calculation\n",
    "def monte_carlo_simulation(df, num_simulations=10000):\n",
    "    total_losses = []\n",
    "    event_counts = []\n",
    "\n",
    "    # Estimate parameters for Poisson distribution (event counts)\n",
    "    event_lambda = df.shape[0] / df['Year'].nunique()  # Average number of events per year\n",
    "\n",
    "    # Estimate parameters for log-normal distribution (severity amounts)\n",
    "    mean_severity = abs(df['Net Loss Amount']).mean()  # Mean severity\n",
    "    stddev_severity = df['Net Loss Amount'].std()  # Standard deviation of severity\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Simulate number of events using Poisson distribution\n",
    "        num_events = np.random.poisson(event_lambda)\n",
    "\n",
    "        # Simulate severity amounts using log-normal distribution\n",
    "        if num_events > 0:\n",
    "            severity = np.random.lognormal(mean=np.log(mean_severity), sigma=stddev_severity, size=num_events)\n",
    "            total_loss = np.sum(severity)\n",
    "        else:\n",
    "            total_loss = 0  # No events means no loss\n",
    "        \n",
    "        total_losses.append(total_loss)\n",
    "        event_counts.append(num_events)\n",
    "\n",
    "    return total_losses, event_counts\n",
    "\n",
    "# Group by Year, Business Line, and Event Type for simulation\n",
    "results = []\n",
    "\n",
    "for (year, business_line, event_type), group in df.groupby(['Year', 'Business Line', 'Event Type']):\n",
    "    simulated_losses, simulated_counts = monte_carlo_simulation(group, num_simulations=10000)\n",
    "    \n",
    "    # Calculate VaR at 99.9% confidence level\n",
    "    var_99_9_loss = np.percentile(simulated_losses, 99.9)\n",
    "    var_99_9_counts = np.percentile(simulated_counts, 99.9)\n",
    "\n",
    "    # Total VaR for Losses and Counts\n",
    "    total_var_loss = np.sum(simulated_losses)\n",
    "    total_var_counts = np.sum(simulated_counts)\n",
    "\n",
    "    # Calculate percentage distributions\n",
    "    var_loss_percentage_distribution = (var_99_9_loss / total_var_loss) * 100 if total_var_loss != 0 else 0\n",
    "    var_counts_percentage_distribution = (var_99_9_counts / total_var_counts) * 100 if total_var_counts != 0 else 0\n",
    "\n",
    "    # Append results to the results list\n",
    "    results.append({\n",
    "        'Year': year,\n",
    "        'Business Line': business_line,\n",
    "        'Event Type': event_type,\n",
    "        'VaR (99.9%) for Loss': var_99_9_loss,\n",
    "        'VaR (99.9%) for Counts': var_99_9_counts,\n",
    "        'VaR Percentage Distribution for Loss': var_loss_percentage_distribution,\n",
    "        'VaR Percentage Distribution for Counts': var_counts_percentage_distribution\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(final_results_df)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "final_results_df.to_csv('monte_carlo_var_results_by_category.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c47a94-3936-4d6c-96ec-e17b908acb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Business Line</th>\n",
       "      <th>Event Type</th>\n",
       "      <th>VaR (99.9%) for Loss</th>\n",
       "      <th>VaR (99.9%) for Counts</th>\n",
       "      <th>VaR Percentage Distribution for Loss</th>\n",
       "      <th>VaR Percentage Distribution for Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Cyber Attack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Market Risk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Natural Disaster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Operational Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Regulatory Violation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>System Failure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Theft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Vendor Risk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year      Business Line            Event Type  VaR (99.9%) for Loss  \\\n",
       "0    2020   Asset Management            Compliance                   NaN   \n",
       "1    2020   Asset Management          Cyber Attack                   NaN   \n",
       "2    2020   Asset Management                 Fraud                   NaN   \n",
       "3    2020   Asset Management           Market Risk                   NaN   \n",
       "4    2020   Asset Management      Natural Disaster                   NaN   \n",
       "..    ...                ...                   ...                   ...   \n",
       "494  2024  Wealth Management     Operational Error                   NaN   \n",
       "495  2024  Wealth Management  Regulatory Violation                   NaN   \n",
       "496  2024  Wealth Management        System Failure                   NaN   \n",
       "497  2024  Wealth Management                 Theft                   NaN   \n",
       "498  2024  Wealth Management           Vendor Risk                   NaN   \n",
       "\n",
       "     VaR (99.9%) for Counts  VaR Percentage Distribution for Loss  \\\n",
       "0                     8.000                                   NaN   \n",
       "1                    10.000                                   NaN   \n",
       "2                    11.000                                   NaN   \n",
       "3                     9.001                                   NaN   \n",
       "4                     9.000                                   NaN   \n",
       "..                      ...                                   ...   \n",
       "494                  46.000                                   NaN   \n",
       "495                  43.001                                   NaN   \n",
       "496                  43.000                                   NaN   \n",
       "497                  28.000                                   NaN   \n",
       "498                  30.000                                   NaN   \n",
       "\n",
       "     VaR Percentage Distribution for Counts  \n",
       "0                                  0.039944  \n",
       "1                                  0.032858  \n",
       "2                                  0.027310  \n",
       "3                                  0.029959  \n",
       "4                                  0.029858  \n",
       "..                                      ...  \n",
       "494                                0.016457  \n",
       "495                                0.016553  \n",
       "496                                0.016525  \n",
       "497                                0.018658  \n",
       "498                                0.018729  \n",
       "\n",
       "[499 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1e7fa74-9334-47e7-8a9b-93690dbb7a78",
   "metadata": {},
   "source": [
    "The Monte Carlo simulation approach is widely used in risk assessment and financial modeling due to its flexibility and ability to model uncertainty through random sampling. However, there are several alternatives for calculating Value at Risk (VaR) that may be more appropriate depending on the context, data characteristics, and specific requirements. Here are some alternatives along with their advantages and disadvantages:\n",
    "\n",
    "1. Historical Simulation\n",
    "Description: This method uses actual historical loss data to compute VaR. It ranks historical losses and identifies the loss at the desired percentile.\n",
    "Advantages:\n",
    "Simple and straightforward to implement.\n",
    "Uses real data, making it intuitive and often reliable if historical data is representative of future risks.\n",
    "Disadvantages:\n",
    "Assumes that future risk patterns will mirror past patterns.\n",
    "Sensitive to outliers and may not capture tail risks adequately.\n",
    "2. Parametric VaR (Variance-Covariance Method)\n",
    "Description: This method assumes a normal (or another specified) distribution for returns and calculates VaR using the mean and standard deviation. For example, for a normal distribution, VaR can be calculated as:\n",
    "VaR\n",
    "=\n",
    "ùúá\n",
    "+\n",
    "ùëß\n",
    "‚ãÖ\n",
    "ùúé\n",
    "VaR=Œº+z‚ãÖœÉ\n",
    "where \n",
    "ùúá\n",
    "Œº is the mean, \n",
    "ùúé\n",
    "œÉ is the standard deviation, and \n",
    "ùëß\n",
    "z is the z-score corresponding to the desired confidence level.\n",
    "Advantages:\n",
    "Quick and computationally efficient.\n",
    "Easily interpretable with the use of statistical properties.\n",
    "Disadvantages:\n",
    "Assumes normality, which may not hold true for financial data (often exhibits fat tails).\n",
    "May underestimate the risk of extreme events (tail risk).\n",
    "3. Extreme Value Theory (EVT)\n",
    "Description: EVT focuses on modeling the tails of the distribution, allowing for a more accurate assessment of risk in extreme scenarios. This can be particularly useful for financial returns that exhibit heavy tails.\n",
    "Advantages:\n",
    "Better captures the risk of extreme events compared to normal distributions.\n",
    "Provides more robust estimates of VaR in heavy-tailed distributions.\n",
    "Disadvantages:\n",
    "Requires careful selection of the underlying distribution and parameters.\n",
    "More complex to implement and interpret.\n",
    "4. Stress Testing and Scenario Analysis\n",
    "Description: This approach involves creating hypothetical scenarios or stress tests to assess the impact of extreme but plausible events on portfolio value. It‚Äôs often used alongside other methods.\n",
    "Advantages:\n",
    "Helps understand potential impacts of rare but severe events.\n",
    "Can be tailored to specific risks relevant to the organization.\n",
    "Disadvantages:\n",
    "May be subjective and depend on the scenarios chosen.\n",
    "Does not provide a probabilistic measure of risk.\n",
    "5. Machine Learning Approaches\n",
    "Description: Advanced techniques, such as decision trees, neural networks, or ensemble methods, can be trained on historical data to predict losses and calculate VaR.\n",
    "Advantages:\n",
    "Can capture complex patterns in the data.\n",
    "May yield more accurate estimates in some contexts.\n",
    "Disadvantages:\n",
    "Requires a substantial amount of data and careful feature engineering.\n",
    "Results can be difficult to interpret.\n",
    "Choosing the Right Approach\n",
    "The choice of method depends on several factors:\n",
    "\n",
    "Data Availability: The amount and quality of historical data can influence which method is appropriate.\n",
    "Risk Profile: Different methods might be better suited for specific types of risks (e.g., market risk, credit risk).\n",
    "Computational Resources: Some methods require more computational power than others.\n",
    "Regulatory Requirements: Some industries have specific guidelines for risk calculation that dictate certain methodologies.\n",
    "In practice, many organizations use a combination of these methods to triangulate their risk estimates and to obtain a more comprehensive view of potential risks. Each method has its strengths and weaknesses, and understanding these can help in making informed decisions about risk management.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "for this problem which is best\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "For the problem of calculating Value at Risk (VaR) based on event counts and severity amounts (losses) in your dataset, the best approach depends on your specific requirements, data characteristics, and the level of detail you want in your risk assessment. Here's an analysis of the methods in the context of your dataset:\n",
    "\n",
    "1. Monte Carlo Simulation\n",
    "Pros:\n",
    "Flexible and can model various types of distributions for both severity (log-normal) and frequency (Poisson).\n",
    "Allows for capturing the joint distribution of event counts and loss severity, providing a comprehensive view of risk.\n",
    "Cons:\n",
    "Computationally intensive, especially with a large number of simulations.\n",
    "Requires careful selection of parameters for the distributions used.\n",
    "2. Historical Simulation\n",
    "Pros:\n",
    "Straightforward and uses actual past data, making it easy to implement and understand.\n",
    "Provides direct insights based on real events.\n",
    "Cons:\n",
    "May not adequately capture future risk if historical patterns do not hold.\n",
    "Sensitive to outliers and extreme events.\n",
    "3. Parametric VaR (Variance-Covariance Method)\n",
    "Pros:\n",
    "Quick and computationally efficient, allowing for fast calculations.\n",
    "Simple to implement and interpret.\n",
    "Cons:\n",
    "Assumes normality for returns, which may not be appropriate for your loss data (which can be skewed).\n",
    "Can underestimate risk in extreme scenarios.\n",
    "4. Extreme Value Theory (EVT)\n",
    "Pros:\n",
    "Specifically designed for modeling tail risks, making it suitable for financial losses that may exhibit heavy tails.\n",
    "Provides robust estimates for extreme events.\n",
    "Cons:\n",
    "More complex to implement and requires careful parameter estimation.\n",
    "May not fully utilize the data if not all aspects of the distribution are considered.\n",
    "5. Stress Testing and Scenario Analysis\n",
    "Pros:\n",
    "Helps assess the impact of extreme but plausible scenarios, providing insights into potential vulnerabilities.\n",
    "Useful for regulatory compliance and risk management discussions.\n",
    "Cons:\n",
    "Can be subjective based on chosen scenarios.\n",
    "Does not yield a probabilistic measure of risk.\n",
    "6. Machine Learning Approaches\n",
    "Pros:\n",
    "Can capture complex relationships and patterns in the data.\n",
    "Potentially more accurate estimates in certain contexts.\n",
    "Cons:\n",
    "Requires a large amount of data and can be resource-intensive.\n",
    "Interpretability may be an issue, especially with complex models.\n",
    "Recommendation\n",
    "Given your dataset, the Monte Carlo simulation approach would likely be the most comprehensive and effective for calculating VaR, as it allows you to model both the frequency of events (using a Poisson distribution) and the severity of losses (using a log-normal distribution). This combination will give you a more holistic view of potential risks and the correlation between event counts and severity.\n",
    "\n",
    "If you're looking for a simpler alternative and have sufficient historical data that you believe is representative of future risks, Historical Simulation could also be a valid choice. However, keep in mind its limitations in capturing tail risks.\n",
    "\n",
    "In summary, if your focus is on accurately modeling the joint distribution of event counts and losses while considering the variability in outcomes, Monte Carlo simulation is the best method for this problem. If computational efficiency and ease of implementation are more critical, consider using Parametric VaR or Historical Simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d7251-4012-45bf-ad89-e5c7c238ae2a",
   "metadata": {},
   "source": [
    "Given your dataset, the Monte Carlo simulation approach would likely be the most comprehensive and effective for calculating VaR, as it allows you to model both the frequency of events (using a Poisson distribution) and the severity of losses (using a log-normal distribution). can we use other distribution \n",
    "\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Yes, while the Monte Carlo simulation approach typically employs a Poisson distribution for modeling event frequency and a log-normal distribution for loss severity, other distributions can also be used depending on the characteristics of your data and the specific requirements of your analysis. Here are some alternative distributions you can consider for both frequency and severity modeling in the context of calculating Value at Risk (VaR):\n",
    "\n",
    "1. Alternative Distributions for Event Frequency\n",
    "Negative Binomial Distribution:\n",
    "\n",
    "Useful if you expect over-dispersion in the count of events (where the variance exceeds the mean).\n",
    "Can model situations with more variability than a Poisson distribution can accommodate.\n",
    "Geometric Distribution:\n",
    "\n",
    "Can be used for modeling the number of trials until the first success, particularly when events occur sequentially.\n",
    "2. Alternative Distributions for Loss Severity\n",
    "Exponential Distribution:\n",
    "\n",
    "Suitable for modeling time until an event occurs, often used in reliability analysis; it can also model loss severity in certain contexts, particularly for non-negative losses.\n",
    "Gamma Distribution:\n",
    "\n",
    "Flexible and can model right-skewed data; it can be particularly useful for modeling positive loss amounts.\n",
    "Weibull Distribution:\n",
    "\n",
    "Can model various shapes of data distributions and is often used in survival analysis; it can handle skewed loss data.\n",
    "Log-Logistic Distribution:\n",
    "\n",
    "Useful for modeling variables that are positive and skewed. It is often used in financial risk modeling due to its flexibility.\n",
    "3. Combining Distributions\n",
    "In a Monte Carlo simulation framework, you can also combine different distributions for event frequency and loss severity. For example:\n",
    "\n",
    "Use a Negative Binomial distribution for event counts if you expect higher variability.\n",
    "Pair it with a Gamma distribution or Weibull distribution for severity, especially if your data exhibits characteristics that are not well-captured by the log-normal distribution.\n",
    "Implementation\n",
    "When using alternative distributions, the steps in the Monte Carlo simulation remain largely the same:\n",
    "\n",
    "Model Event Frequency: Sample the number of events using the chosen distribution.\n",
    "Model Loss Severity: For each sampled event, generate a severity amount from the selected distribution.\n",
    "Calculate Total Loss: Aggregate the losses from the simulated events.\n",
    "Repeat: Perform many iterations to create a distribution of potential total losses.\n",
    "Calculate VaR: Determine the VaR from the simulated loss distribution at your desired confidence level.\n",
    "Conclusion\n",
    "The choice of distribution should be guided by the characteristics of your data. Before implementing a specific distribution, it‚Äôs advisable to perform exploratory data analysis (EDA) to understand the distribution of your event counts and loss amounts. This can include visualizations (like histograms or Q-Q plots) and statistical tests to assess fit (like the Kolmogorov-Smirnov test). By selecting the most appropriate distributions based on your data, you can enhance the accuracy and reliability of your VaR estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97715c48-a1cb-4ec7-9edd-269e550d11d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715247e-f22e-4bcb-8485-8e9b44219c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a920954-99b3-4c77-8302-c83d87ef80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year      Business Line            Event Type   VaR (99.9%)\n",
      "0    2020   Asset Management            Compliance  60581.204427\n",
      "1    2020   Asset Management          Cyber Attack  63386.398500\n",
      "2    2020   Asset Management                 Fraud  62194.820464\n",
      "3    2020   Asset Management           Market Risk  60664.371067\n",
      "4    2020   Asset Management      Natural Disaster  60917.146685\n",
      "..    ...                ...                   ...           ...\n",
      "493  2024  Wealth Management     Operational Error  60111.701665\n",
      "494  2024  Wealth Management  Regulatory Violation  62387.452358\n",
      "495  2024  Wealth Management        System Failure  64387.351613\n",
      "496  2024  Wealth Management                 Theft  61319.523473\n",
      "497  2024  Wealth Management           Vendor Risk  60305.491302\n",
      "\n",
      "[498 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# The choice of distribution should be guided by the characteristics of your data. Before implementing a specific distribution, it‚Äôs advisable to perform exploratory data analysis (EDA) to understand the distribution of your event counts and loss amounts. This can include visualizations (like histograms or Q-Q plots) and statistical tests to assess fit (like the Kolmogorov-Smirnov test). By selecting the most appropriate distributions based on your data, you can enhance the accuracy and reliability of your VaR estimates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# You said:\n",
    "# use those and build code\n",
    "# ChatGPT said:\n",
    "# ChatGPT\n",
    "# To implement a Monte Carlo simulation approach using alternative distributions for modeling event frequency and loss severity, let's consider using the Negative Binomial distribution for event counts and the Gamma distribution for loss severity. Below is a complete code example based on your dataset that demonstrates how to perform this simulation and calculate the Value at Risk (VaR).\n",
    "\n",
    "# Code Implementation\n",
    "# python\n",
    "# Copy code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "num_simulations = 10000  # Number of Monte Carlo simulations\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Monte Carlo Simulation to calculate VaR\n",
    "def monte_carlo_var(df, num_simulations, confidence_level):\n",
    "    # Initialize an empty list to store total losses for each simulation\n",
    "    total_losses = []\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Sample the number of events using a Negative Binomial distribution\n",
    "        # Parameters for the Negative Binomial distribution can be adjusted\n",
    "        num_events = np.random.negative_binomial(5, 0.5)  # Example parameters (n=5, p=0.5)\n",
    "\n",
    "        # Sample losses using a Gamma distribution\n",
    "        # Parameters for the Gamma distribution can be adjusted\n",
    "        losses = np.random.gamma(2, 1500, num_events)  # Example parameters (shape=2, scale=1500)\n",
    "\n",
    "        # Calculate total loss for this simulation\n",
    "        total_losses.append(np.sum(losses))\n",
    "\n",
    "    # Calculate Value at Risk (VaR)\n",
    "    var_value = np.percentile(total_losses, confidence_level * 100)\n",
    "    return var_value\n",
    "\n",
    "# Set confidence level for VaR\n",
    "confidence_level = 0.999  # 99.9% VaR\n",
    "\n",
    "# Group by Year, Business Line, and Event Type to calculate VaR\n",
    "var_results = []\n",
    "for (year, business_line, event_type), group in df.groupby(['Year', 'Business Line', 'Event Type']):\n",
    "    var = monte_carlo_var(group, num_simulations, confidence_level)\n",
    "    var_results.append({\n",
    "        'Year': year,\n",
    "        'Business Line': business_line,\n",
    "        'Event Type': event_type,\n",
    "        'VaR (99.9%)': var\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# Display the VaR results\n",
    "print(var_results_df)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "var_results_df.to_csv('monte_carlo_var_results.csv', index=False)\n",
    "# Explanation\n",
    "# Data Generation: The code generates a synthetic dataset containing various business lines and event types, along with net loss amounts.\n",
    "\n",
    "# Monte Carlo Simulation:\n",
    "\n",
    "# A Negative Binomial distribution is used to simulate the number of events in each simulation.\n",
    "# A Gamma distribution is used to model the severity of losses per event.\n",
    "# Total losses for each simulation are calculated, and then the VaR at the specified confidence level (99.9%) is derived from the simulated total losses.\n",
    "# Grouping: The results are grouped by year, business line, and event type to provide a comprehensive overview of the VaR across these dimensions.\n",
    "\n",
    "# Output: The final results are printed and can also be saved to a CSV file for further analysis.\n",
    "\n",
    "# Adjusting Parameters\n",
    "# Feel free to adjust the parameters for the distributions based on your specific needs or insights from your data. The choice of parameters can significantly affect the simulation results, so it's important to select them based on the characteristics of your historical data or domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19101f9-ab3d-42f5-9e81-7e6a4b2fef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year      Business Line            Event Type  VaR (99.9%) for Loss  \\\n",
      "0    2020   Asset Management            Compliance          59385.015089   \n",
      "1    2020   Asset Management          Cyber Attack          60934.292744   \n",
      "2    2020   Asset Management           Market Risk          60107.145957   \n",
      "3    2020   Asset Management      Natural Disaster          61423.305848   \n",
      "4    2020   Asset Management     Operational Error          61758.998356   \n",
      "..    ...                ...                   ...                   ...   \n",
      "490  2024  Wealth Management     Operational Error          62160.661861   \n",
      "491  2024  Wealth Management  Regulatory Violation          63410.229956   \n",
      "492  2024  Wealth Management        System Failure          63809.967747   \n",
      "493  2024  Wealth Management                 Theft          61491.693265   \n",
      "494  2024  Wealth Management           Vendor Risk          66251.894892   \n",
      "\n",
      "     VaR (99.9%) for Counts  Average Loss per Event  Average Event Count  \n",
      "0                    19.000            14794.815448               4.9438  \n",
      "1                    18.000            15044.352481               5.0181  \n",
      "2                    18.000            14979.490131               4.9762  \n",
      "3                    18.000            14839.476812               4.9426  \n",
      "4                    19.000            15044.165774               5.0068  \n",
      "..                      ...                     ...                  ...  \n",
      "490                  19.000            14944.414035               4.9939  \n",
      "491                  18.001            14877.332037               4.9597  \n",
      "492                  19.000            15092.846308               5.0172  \n",
      "493                  19.000            15132.766818               5.0553  \n",
      "494                  20.000            15001.034484               5.0228  \n",
      "\n",
      "[495 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "num_simulations = 10000  # Number of Monte Carlo simulations\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Monte Carlo Simulation to calculate VaR\n",
    "def monte_carlo_var(df, num_simulations, confidence_level):\n",
    "    # Initialize lists to store total losses and counts for each simulation\n",
    "    total_losses = []\n",
    "    total_counts = []\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Sample the number of events using a Negative Binomial distribution\n",
    "        num_events = np.random.negative_binomial(5, 0.5)  # Example parameters (n=5, p=0.5)\n",
    "        total_counts.append(num_events)\n",
    "\n",
    "        # Sample losses using a Gamma distribution\n",
    "        losses = np.random.gamma(2, 1500, num_events)  # Example parameters (shape=2, scale=1500)\n",
    "\n",
    "        # Calculate total loss for this simulation\n",
    "        total_losses.append(np.sum(losses))\n",
    "\n",
    "    # Calculate Value at Risk (VaR) for losses\n",
    "    var_value_loss = np.percentile(total_losses, confidence_level * 100)\n",
    "\n",
    "    # Calculate Value at Risk (VaR) for counts\n",
    "    var_value_counts = np.percentile(total_counts, confidence_level * 100)\n",
    "\n",
    "    return var_value_loss, var_value_counts, np.mean(total_losses), np.mean(total_counts)\n",
    "\n",
    "# Set confidence level for VaR\n",
    "confidence_level = 0.999  # 99.9% VaR\n",
    "\n",
    "# Group by Year, Business Line, and Event Type to calculate VaR\n",
    "var_results = []\n",
    "for (year, business_line, event_type), group in df.groupby(['Year', 'Business Line', 'Event Type']):\n",
    "    var_loss, var_counts, avg_loss, avg_counts = monte_carlo_var(group, num_simulations, confidence_level)\n",
    "    var_results.append({\n",
    "        'Year': year,\n",
    "        'Business Line': business_line,\n",
    "        'Event Type': event_type,\n",
    "        'VaR (99.9%) for Loss': var_loss,\n",
    "        'VaR (99.9%) for Counts': var_counts,\n",
    "        'Average Loss per Event': avg_loss,\n",
    "        'Average Event Count': avg_counts\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# Display the VaR results\n",
    "print(var_results_df)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "var_results_df.to_csv('monte_carlo_var_results_extended.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde9b9f3-30a4-46dd-966c-22855a8865a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year      Business Line            Event Type  VaR (99.9%) for Loss  \\\n",
      "0    2020   Asset Management            Compliance          61512.250890   \n",
      "1    2020   Asset Management          Cyber Attack          62120.863831   \n",
      "2    2020   Asset Management                 Fraud          61223.165256   \n",
      "3    2020   Asset Management      Natural Disaster          63854.210776   \n",
      "4    2020   Asset Management     Operational Error          58860.710745   \n",
      "..    ...                ...                   ...                   ...   \n",
      "493  2024  Wealth Management     Operational Error          63649.046603   \n",
      "494  2024  Wealth Management  Regulatory Violation          64262.196192   \n",
      "495  2024  Wealth Management        System Failure          59489.087605   \n",
      "496  2024  Wealth Management                 Theft          62564.447282   \n",
      "497  2024  Wealth Management           Vendor Risk          62179.953159   \n",
      "\n",
      "     VaR (99.9%) for Counts  VaR Percentage Distribution for Loss  \\\n",
      "0                    19.000                              0.195911   \n",
      "1                    19.000                              0.197850   \n",
      "2                    19.000                              0.194990   \n",
      "3                    19.000                              0.203370   \n",
      "4                    18.000                              0.187466   \n",
      "..                      ...                                   ...   \n",
      "493                  18.000                              0.202717   \n",
      "494                  19.000                              0.204669   \n",
      "495                  18.001                              0.189468   \n",
      "496                  20.000                              0.199262   \n",
      "497                  19.000                              0.198038   \n",
      "\n",
      "     VaR Percentage Distribution for Counts  \n",
      "0                                  0.201997  \n",
      "1                                  0.201997  \n",
      "2                                  0.201997  \n",
      "3                                  0.201997  \n",
      "4                                  0.191365  \n",
      "..                                      ...  \n",
      "493                                0.191365  \n",
      "494                                0.201997  \n",
      "495                                0.191376  \n",
      "496                                0.212628  \n",
      "497                                0.201997  \n",
      "\n",
      "[498 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "num_simulations = 10000  # Number of Monte Carlo simulations\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Monte Carlo Simulation to calculate VaR\n",
    "def monte_carlo_var(df, num_simulations, confidence_level):\n",
    "    total_losses = []\n",
    "    total_counts = []\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Sample the number of events using a Negative Binomial distribution\n",
    "        num_events = np.random.negative_binomial(5, 0.5)  # Example parameters (n=5, p=0.5)\n",
    "        total_counts.append(num_events)\n",
    "\n",
    "        # Sample losses using a Gamma distribution\n",
    "        losses = np.random.gamma(2, 1500, num_events)  # Example parameters (shape=2, scale=1500)\n",
    "\n",
    "        total_losses.append(np.sum(losses))\n",
    "\n",
    "    var_value_loss = np.percentile(total_losses, confidence_level * 100)\n",
    "    var_value_counts = np.percentile(total_counts, confidence_level * 100)\n",
    "\n",
    "    return var_value_loss, var_value_counts\n",
    "\n",
    "# Set confidence level for VaR\n",
    "confidence_level = 0.999  # 99.9% VaR\n",
    "\n",
    "# Group by Year, Business Line, and Event Type to calculate VaR\n",
    "var_results = []\n",
    "for (year, business_line, event_type), group in df.groupby(['Year', 'Business Line', 'Event Type']):\n",
    "    var_loss, var_counts = monte_carlo_var(group, num_simulations, confidence_level)\n",
    "    var_results.append({\n",
    "        'Year': year,\n",
    "        'Business Line': business_line,\n",
    "        'Event Type': event_type,\n",
    "        'VaR (99.9%) for Loss': var_loss,\n",
    "        'VaR (99.9%) for Counts': var_counts\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# Calculate total VaR for losses and counts for percentage distribution calculation\n",
    "total_var_loss = var_results_df['VaR (99.9%) for Loss'].sum()\n",
    "total_var_counts = var_results_df['VaR (99.9%) for Counts'].sum()\n",
    "\n",
    "# Calculate percentage distribution\n",
    "var_results_df['VaR Percentage Distribution for Loss'] = (\n",
    "    var_results_df['VaR (99.9%) for Loss'] / total_var_loss * 100\n",
    ")\n",
    "\n",
    "var_results_df['VaR Percentage Distribution for Counts'] = (\n",
    "    var_results_df['VaR (99.9%) for Counts'] / total_var_counts * 100\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(var_results_df)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "var_results_df.to_csv('monte_carlo_var_results_with_percentages.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a19bb3db-fdb4-4068-88f1-42be7b3fa3f0",
   "metadata": {},
   "source": [
    "Step-by-Step Approach\n",
    "Calculate VaR for Loss and Event Counts: This has already been implemented in the previous code.\n",
    "Compute Total VaR for Losses and Counts: Once we have individual VaRs, we will sum them up to get the total VaR.\n",
    "Calculate the Percentage Distribution: For each entry, compute the percentage distribution as:\n",
    "VaR Percentage Distribution for Loss: \n",
    "VaR¬†for¬†Loss\n",
    "Total¬†VaR¬†for¬†Loss\n",
    "√ó\n",
    "100\n",
    "Total¬†VaR¬†for¬†Loss\n",
    "VaR¬†for¬†Loss\n",
    "‚Äã\n",
    " √ó100\n",
    "VaR Percentage Distribution for Counts: \n",
    "VaR¬†for¬†Counts\n",
    "Total¬†VaR¬†for¬†Counts\n",
    "√ó\n",
    "100\n",
    "Total¬†VaR¬†for¬†Counts\n",
    "VaR¬†for¬†Counts\n",
    "‚Äã\n",
    " √ó100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76badacc-2572-45a8-bdca-4a5ec57fa288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Business Line</th>\n",
       "      <th>Event Type</th>\n",
       "      <th>VaR (99.9%) for Loss</th>\n",
       "      <th>VaR (99.9%) for Counts</th>\n",
       "      <th>VaR Percentage Distribution for Loss</th>\n",
       "      <th>VaR Percentage Distribution for Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>61512.250890</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.195911</td>\n",
       "      <td>0.201997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Cyber Attack</td>\n",
       "      <td>62120.863831</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.197850</td>\n",
       "      <td>0.201997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>61223.165256</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.194990</td>\n",
       "      <td>0.201997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Natural Disaster</td>\n",
       "      <td>63854.210776</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.203370</td>\n",
       "      <td>0.201997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>Asset Management</td>\n",
       "      <td>Operational Error</td>\n",
       "      <td>58860.710745</td>\n",
       "      <td>18.000</td>\n",
       "      <td>0.187466</td>\n",
       "      <td>0.191365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Operational Error</td>\n",
       "      <td>63649.046603</td>\n",
       "      <td>18.000</td>\n",
       "      <td>0.202717</td>\n",
       "      <td>0.191365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Regulatory Violation</td>\n",
       "      <td>64262.196192</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.204669</td>\n",
       "      <td>0.201997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>System Failure</td>\n",
       "      <td>59489.087605</td>\n",
       "      <td>18.001</td>\n",
       "      <td>0.189468</td>\n",
       "      <td>0.191376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Theft</td>\n",
       "      <td>62564.447282</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.199262</td>\n",
       "      <td>0.212628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wealth Management</td>\n",
       "      <td>Vendor Risk</td>\n",
       "      <td>62179.953159</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.198038</td>\n",
       "      <td>0.201997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year      Business Line            Event Type  VaR (99.9%) for Loss  \\\n",
       "0    2020   Asset Management            Compliance          61512.250890   \n",
       "1    2020   Asset Management          Cyber Attack          62120.863831   \n",
       "2    2020   Asset Management                 Fraud          61223.165256   \n",
       "3    2020   Asset Management      Natural Disaster          63854.210776   \n",
       "4    2020   Asset Management     Operational Error          58860.710745   \n",
       "..    ...                ...                   ...                   ...   \n",
       "493  2024  Wealth Management     Operational Error          63649.046603   \n",
       "494  2024  Wealth Management  Regulatory Violation          64262.196192   \n",
       "495  2024  Wealth Management        System Failure          59489.087605   \n",
       "496  2024  Wealth Management                 Theft          62564.447282   \n",
       "497  2024  Wealth Management           Vendor Risk          62179.953159   \n",
       "\n",
       "     VaR (99.9%) for Counts  VaR Percentage Distribution for Loss  \\\n",
       "0                    19.000                              0.195911   \n",
       "1                    19.000                              0.197850   \n",
       "2                    19.000                              0.194990   \n",
       "3                    19.000                              0.203370   \n",
       "4                    18.000                              0.187466   \n",
       "..                      ...                                   ...   \n",
       "493                  18.000                              0.202717   \n",
       "494                  19.000                              0.204669   \n",
       "495                  18.001                              0.189468   \n",
       "496                  20.000                              0.199262   \n",
       "497                  19.000                              0.198038   \n",
       "\n",
       "     VaR Percentage Distribution for Counts  \n",
       "0                                  0.201997  \n",
       "1                                  0.201997  \n",
       "2                                  0.201997  \n",
       "3                                  0.201997  \n",
       "4                                  0.191365  \n",
       "..                                      ...  \n",
       "493                                0.191365  \n",
       "494                                0.201997  \n",
       "495                                0.191376  \n",
       "496                                0.212628  \n",
       "497                                0.201997  \n",
       "\n",
       "[498 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_results_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d1c6a2d-fb1f-4374-a0fc-a93ec7b8f5f4",
   "metadata": {},
   "source": [
    "Explanation of New Columns\n",
    "VaR (99.9%) for Loss: The calculated Value at Risk for the loss amounts at the 99.9% confidence level.\n",
    "VaR (99.9%) for Counts: The calculated Value at Risk for the event counts at the 99.9% confidence level.\n",
    "VaR Percentage Distribution for Loss: The percentage of each business line and event type's VaR for loss relative to the total VaR for losses.\n",
    "VaR Percentage Distribution for Counts: The percentage of each business line and event type's VaR for event counts relative to the total VaR for counts.\n",
    "Output\n",
    "The resulting DataFrame var_results_df will provide a comprehensive overview of the risk metrics, including the required percentage distributions for both loss and counts. This information can be useful for further analysis and risk management strategies. You can also save the results to a CSV file for reporting or future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a01d6d-89f6-414f-b262-58cc2cacae61",
   "metadata": {},
   "source": [
    "Monte Carlo simulation is considered a strong method for estimating Value at Risk (VaR) and is widely used in finance and risk management for several reasons:\n",
    "\n",
    "Advantages of Monte Carlo Simulation\n",
    "Flexibility: It can accommodate a wide range of distributions for both the frequency of events (e.g., using Poisson or Negative Binomial distributions) and the severity of losses (e.g., using log-normal or Gamma distributions). This flexibility allows it to model complex scenarios more realistically than simpler methods.\n",
    "\n",
    "Realism in Assumptions: Monte Carlo simulations can incorporate different types of risks and correlations between various risk factors, leading to more accurate risk assessments. You can model different scenarios (both normal and extreme events) which can provide insights into tail risks.\n",
    "\n",
    "Robustness: By running thousands (or millions) of simulations, the results are generally robust, especially when aggregated. This can help smooth out anomalies or outlier results that might skew simpler methods.\n",
    "\n",
    "Detailed Insights: It provides a complete distribution of potential outcomes, not just a point estimate of VaR. This allows for the analysis of potential losses at various confidence levels, providing more comprehensive risk insights.\n",
    "\n",
    "Visual Representation: Results from Monte Carlo simulations can be visualized in various ways (histograms, cumulative distribution functions), making it easier to communicate risks to stakeholders.\n",
    "\n",
    "Limitations of Monte Carlo Simulation\n",
    "Computational Intensity: It can be computationally expensive and time-consuming, especially with a high number of simulations and complex models. This may require significant processing power, especially in large organizations or when dealing with big datasets.\n",
    "\n",
    "Model Risk: The accuracy of the results heavily depends on the assumptions made about the underlying distributions and the correlations between variables. Poorly specified models can lead to misleading results.\n",
    "\n",
    "Data Requirements: For effective implementation, it often requires high-quality historical data to estimate parameters accurately, which can be a challenge in practice.\n",
    "\n",
    "Interpretation of Results: The outputs can be complex and may require expert interpretation to derive meaningful insights.\n",
    "\n",
    "Alternatives to Monte Carlo Simulation\n",
    "Historical Simulation: This approach uses actual historical data to estimate potential future losses, providing a simple and intuitive way to compute VaR but is limited by the historical period considered.\n",
    "\n",
    "Parametric Methods: These rely on statistical parameters (mean and standard deviation) to calculate VaR assuming normal distribution. This method is quick but may not adequately capture the tail risks associated with financial data.\n",
    "\n",
    "Bootstrapping: This method involves resampling from the historical data to create simulated datasets. It can capture the empirical distribution of returns but may still miss outlier events not captured in the historical period.\n",
    "\n",
    "Stress Testing and Scenario Analysis: These techniques involve assessing the impact of extreme but plausible adverse conditions on portfolios, which can complement Monte Carlo methods.\n",
    "\n",
    "Conclusion\n",
    "In summary, Monte Carlo simulation is a powerful tool for calculating VaR, especially when dealing with complex risk scenarios involving multiple variables. However, it should be used alongside other methods to provide a comprehensive risk management framework. Depending on the specific requirements of your organization and the complexity of your risk environment, combining Monte Carlo with other approaches can lead to better decision-making and risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8106fc-a37d-451a-9e98-6067f27dce7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6dda706-8274-4859-8565-f71a8e744cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at Risk Results:\n",
      "Monte Carlo VaR (99.9%): 0.0\n",
      "Historical Simulation VaR (99.9%): -9961.154187904061\n",
      "Parametric VaR (99.9%): -17720.222346627463\n",
      "\n",
      "Comparison of VaR Results:\n",
      "                  Method   VaR (99.9%)\n",
      "0            Monte Carlo      0.000000\n",
      "1  Historical Simulation  -9961.154188\n",
      "2             Parametric -17720.222347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_32220\\1983422247.py:64: RuntimeWarning: overflow encountered in multiply\n",
      "  total_losses = event_counts * loss_amounts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# 1. Monte Carlo Simulation for VaR\n",
    "def monte_carlo_var(data, confidence_level=0.999, simulations=10000):\n",
    "    # Generate random event counts using Poisson distribution\n",
    "    event_counts = np.random.poisson(data['Event Type'].count() / len(data) * simulations)\n",
    "    # Generate random loss amounts using log-normal distribution\n",
    "    loss_amounts = np.random.lognormal(mean=np.mean(data['Net Loss Amount']), \n",
    "                                        sigma=np.std(data['Net Loss Amount']), \n",
    "                                        size=simulations)\n",
    "    # Calculate total loss for each simulation\n",
    "    total_losses = event_counts * loss_amounts\n",
    "    # Calculate VaR\n",
    "    var = np.percentile(total_losses, (1 - confidence_level) * 100)\n",
    "    return var\n",
    "\n",
    "# 2. Historical Simulation for VaR\n",
    "def historical_var(data, confidence_level=0.999):\n",
    "    # Calculate empirical losses and return the VaR\n",
    "    losses = data['Net Loss Amount']\n",
    "    var = np.percentile(losses, (1 - confidence_level) * 100)\n",
    "    return var\n",
    "\n",
    "# 3. Parametric VaR assuming normal distribution\n",
    "def parametric_var(data, confidence_level=0.999):\n",
    "    mean = np.mean(data['Net Loss Amount'])\n",
    "    std_dev = np.std(data['Net Loss Amount'])\n",
    "    # Calculate VaR using normal distribution\n",
    "    var = mean + std_dev * stats.norm.ppf(1 - confidence_level)\n",
    "    return var\n",
    "\n",
    "# Calculate VaR for each method\n",
    "confidence_level = 0.999\n",
    "monte_carlo_var_result = monte_carlo_var(df, confidence_level)\n",
    "historical_var_result = historical_var(df, confidence_level)\n",
    "parametric_var_result = parametric_var(df, confidence_level)\n",
    "\n",
    "# Print the results\n",
    "print(\"Value at Risk Results:\")\n",
    "print(f\"Monte Carlo VaR (99.9%): {monte_carlo_var_result}\")\n",
    "print(f\"Historical Simulation VaR (99.9%): {historical_var_result}\")\n",
    "print(f\"Parametric VaR (99.9%): {parametric_var_result}\")\n",
    "\n",
    "# Group the results for better comparison\n",
    "results = {\n",
    "    'Method': ['Monte Carlo', 'Historical Simulation', 'Parametric'],\n",
    "    'VaR (99.9%)': [monte_carlo_var_result, historical_var_result, parametric_var_result]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of VaR Results:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f60da1-1d9f-4798-8b3a-a835391767c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_32220\\1914804469.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  empirical_results = df.groupby(['Year', 'Business Line', 'Event Type']).apply(calculate_var_and_distribution).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year      Business Line            Event Type  \\\n",
      "0    2020   Asset Management            Compliance   \n",
      "1    2020   Asset Management          Cyber Attack   \n",
      "2    2020   Asset Management                 Fraud   \n",
      "3    2020   Asset Management           Market Risk   \n",
      "4    2020   Asset Management      Natural Disaster   \n",
      "..    ...                ...                   ...   \n",
      "493  2024  Wealth Management     Operational Error   \n",
      "494  2024  Wealth Management  Regulatory Violation   \n",
      "495  2024  Wealth Management        System Failure   \n",
      "496  2024  Wealth Management                 Theft   \n",
      "497  2024  Wealth Management           Vendor Risk   \n",
      "\n",
      "     VaR (99.9%) for Loss (Empirical)  VaR (99.9%) for Counts (Empirical)  \\\n",
      "0                        -9798.360137                                 5.0   \n",
      "1                        -8026.426078                                 3.0   \n",
      "2                        -7867.590607                                 3.0   \n",
      "3                         3396.062963                                 2.0   \n",
      "4                        -9227.423250                                 5.0   \n",
      "..                                ...                                 ...   \n",
      "493                      -9038.913654                                16.0   \n",
      "494                      -7509.705183                                14.0   \n",
      "495                      -9392.539346                                18.0   \n",
      "496                      -9581.858814                                22.0   \n",
      "497                      -7686.684908                                19.0   \n",
      "\n",
      "     VaR Percentage Distribution for Loss (Empirical)  \\\n",
      "0                                          123.205717   \n",
      "1                                          100.925213   \n",
      "2                                           98.927997   \n",
      "3                                          -42.702490   \n",
      "4                                          116.026690   \n",
      "..                                                ...   \n",
      "493                                        113.656349   \n",
      "494                                         94.427904   \n",
      "495                                        118.102879   \n",
      "496                                        120.483404   \n",
      "497                                         96.653268   \n",
      "\n",
      "     VaR Percentage Distribution for Counts (Empirical)  \\\n",
      "0                                                 0.05    \n",
      "1                                                 0.03    \n",
      "2                                                 0.03    \n",
      "3                                                 0.02    \n",
      "4                                                 0.05    \n",
      "..                                                 ...    \n",
      "493                                               0.16    \n",
      "494                                               0.14    \n",
      "495                                               0.18    \n",
      "496                                               0.22    \n",
      "497                                               0.19    \n",
      "\n",
      "     VaR (99.9%) for Loss (Monte Carlo)  VaR (99.9%) for Counts (Monte Carlo)  \\\n",
      "0                          1.229915e+05                                13.000   \n",
      "1                          1.287586e+05                                10.000   \n",
      "2                          1.855577e+05                                10.000   \n",
      "3                                   NaN                                 8.000   \n",
      "4                          1.177447e+05                                13.000   \n",
      "..                                  ...                                   ...   \n",
      "493                        5.325144e+06                                29.000   \n",
      "494                        2.901080e+05                                27.000   \n",
      "495                        8.314983e+05                                33.000   \n",
      "496                        2.646933e+06                                38.000   \n",
      "497                        4.245206e+05                                34.001   \n",
      "\n",
      "     VaR Percentage Distribution for Loss  \\\n",
      "0                                0.007154   \n",
      "1                                0.007490   \n",
      "2                                0.010794   \n",
      "3                                0.000198   \n",
      "4                                0.006849   \n",
      "..                                    ...   \n",
      "493                              0.309764   \n",
      "494                              0.016876   \n",
      "495                              0.048368   \n",
      "496                              0.153972   \n",
      "497                              0.024694   \n",
      "\n",
      "     VaR Percentage Distribution for Counts  \n",
      "0                                  0.075476  \n",
      "1                                  0.058058  \n",
      "2                                  0.058058  \n",
      "3                                  0.046447  \n",
      "4                                  0.075476  \n",
      "..                                      ...  \n",
      "493                                0.168369  \n",
      "494                                0.156757  \n",
      "495                                0.191592  \n",
      "496                                0.220621  \n",
      "497                                0.197404  \n",
      "\n",
      "[498 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu Singh\\AppData\\Local\\Temp\\ipykernel_32220\\1914804469.py:98: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  monte_carlo_results = df.groupby(['Year', 'Business Line', 'Event Type']).apply(monte_carlo_simulation).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Function to generate random dates\n",
    "def random_dates(start, end, n=10):\n",
    "    return [start + timedelta(days=np.random.randint(0, (end - start).days)) for _ in range(n)]\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "start_date = datetime.now() - timedelta(days=4*365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Expanded categories\n",
    "business_lines = [\n",
    "    \"Retail\", \"Corporate Banking\", \"Investment Banking\", \"Insurance\",\n",
    "    \"Wealth Management\", \"Asset Management\", \"Private Banking\",\n",
    "    \"Credit Card Services\", \"Mortgage Lending\", \"Financial Advisory\"\n",
    "]\n",
    "\n",
    "event_types = [\n",
    "    \"Fraud\", \"System Failure\", \"Theft\", \"Compliance\", \"Natural Disaster\",\n",
    "    \"Cyber Attack\", \"Market Risk\", \"Operational Error\", \"Vendor Risk\", \"Regulatory Violation\"\n",
    "]\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"Date\": random_dates(start_date, end_date, num_records),\n",
    "    \"Unique Event ID\": [f\"EID{str(i).zfill(5)}\" for i in range(num_records)],\n",
    "    \"Event Type\": np.random.choice(event_types, num_records),\n",
    "    \"Business Line\": np.random.choice(business_lines, num_records),\n",
    "    \"Event Description\": np.random.choice(\n",
    "        [\n",
    "            \"Unauthorized transaction\", \"Server downtime\", \"Lost assets\", \n",
    "            \"Regulatory fines\", \"Data breach\", \"Network failure\", \n",
    "            \"Inadequate compliance\", \"Financial misstatement\", \n",
    "            \"Supplier issues\", \"Internal fraud\"\n",
    "        ],\n",
    "        num_records\n",
    "    ),\n",
    "    \"Net Loss Amount\": np.random.choice(\n",
    "        [np.random.uniform(-10000, 0) for _ in range(num_records // 2)] + \n",
    "        [np.random.uniform(0, 10000) for _ in range(num_records // 2)],\n",
    "        num_records\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Function to calculate VaR and percentage distributions\n",
    "def calculate_var_and_distribution(group):\n",
    "    # Calculate VaR for Loss (empirical method)\n",
    "    var_loss = np.percentile(group['Net Loss Amount'], 0.1)  # 99.9% VaR\n",
    "    var_counts = group['Unique Event ID'].count()  # Total events in the group\n",
    "    \n",
    "    # Calculate percentage distributions\n",
    "    total_var_loss = df['Net Loss Amount'].quantile(0.1)\n",
    "    total_event_counts = df['Unique Event ID'].count()\n",
    "    \n",
    "    percentage_distribution_loss = (var_loss / total_var_loss) * 100 if total_var_loss != 0 else 0\n",
    "    percentage_distribution_counts = (var_counts / total_event_counts) * 100 if total_event_counts != 0 else 0\n",
    "    \n",
    "    return pd.Series({\n",
    "        'VaR (99.9%) for Loss (Empirical)': var_loss,\n",
    "        'VaR (99.9%) for Counts (Empirical)': var_counts,\n",
    "        'VaR Percentage Distribution for Loss (Empirical)': percentage_distribution_loss,\n",
    "        'VaR Percentage Distribution for Counts (Empirical)': percentage_distribution_counts\n",
    "    })\n",
    "\n",
    "# Monte Carlo Simulation Method\n",
    "def monte_carlo_simulation(group, num_simulations=10000):\n",
    "    # Event counts follow a Poisson distribution\n",
    "    event_count_mean = group['Unique Event ID'].count()\n",
    "    simulated_counts = np.random.poisson(event_count_mean, num_simulations)\n",
    "    \n",
    "    # Severity amounts follow a log-normal distribution\n",
    "    log_severity = np.log(-group['Net Loss Amount'][group['Net Loss Amount'] < 0].dropna())  # only negative losses\n",
    "    simulated_losses = np.random.lognormal(mean=log_severity.mean(), sigma=log_severity.std(), size=num_simulations)\n",
    "    \n",
    "    total_losses = simulated_counts * simulated_losses\n",
    "    \n",
    "    # Calculate VaR for losses and counts\n",
    "    var_loss_mc = np.percentile(total_losses, 99.9)\n",
    "    var_counts_mc = np.percentile(simulated_counts, 99.9)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'VaR (99.9%) for Loss (Monte Carlo)': var_loss_mc,\n",
    "        'VaR (99.9%) for Counts (Monte Carlo)': var_counts_mc\n",
    "    })\n",
    "\n",
    "# Group by Year, Business Line, and Event Type\n",
    "empirical_results = df.groupby(['Year', 'Business Line', 'Event Type']).apply(calculate_var_and_distribution).reset_index()\n",
    "monte_carlo_results = df.groupby(['Year', 'Business Line', 'Event Type']).apply(monte_carlo_simulation).reset_index()\n",
    "\n",
    "# Merge the results\n",
    "final_results = pd.merge(empirical_results, monte_carlo_results, on=['Year', 'Business Line', 'Event Type'])\n",
    "\n",
    "# Calculate total VaR and percentage distributions for the combined results\n",
    "final_results['Total VaR Loss'] = final_results[['VaR (99.9%) for Loss (Empirical)', 'VaR (99.9%) for Loss (Monte Carlo)']].max(axis=1)\n",
    "final_results['Total VaR Counts'] = final_results[['VaR (99.9%) for Counts (Empirical)', 'VaR (99.9%) for Counts (Monte Carlo)']].max(axis=1)\n",
    "\n",
    "# Calculate percentage distributions\n",
    "total_var_loss = final_results['Total VaR Loss'].sum()\n",
    "total_var_counts = final_results['Total VaR Counts'].sum()\n",
    "\n",
    "final_results['VaR Percentage Distribution for Loss'] = (final_results['Total VaR Loss'] / total_var_loss) * 100\n",
    "final_results['VaR Percentage Distribution for Counts'] = (final_results['Total VaR Counts'] / total_var_counts) * 100\n",
    "\n",
    "# Final result columns\n",
    "final_results = final_results[['Year', 'Business Line', 'Event Type',\n",
    "                               'VaR (99.9%) for Loss (Empirical)', \n",
    "                               'VaR (99.9%) for Counts (Empirical)',\n",
    "                               'VaR Percentage Distribution for Loss (Empirical)',\n",
    "                               'VaR Percentage Distribution for Counts (Empirical)',\n",
    "                               'VaR (99.9%) for Loss (Monte Carlo)', \n",
    "                               'VaR (99.9%) for Counts (Monte Carlo)',\n",
    "                               'VaR Percentage Distribution for Loss', \n",
    "                               'VaR Percentage Distribution for Counts']]\n",
    "\n",
    "# Display the final results\n",
    "print(final_results)\n",
    "\n",
    "# Optionally, save results to a CSV\n",
    "final_results.to_csv('var_results_all_methods.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1958b0cf-7c0a-434d-87be-c3aa6a6fd7a3",
   "metadata": {},
   "source": [
    "Explanation of the Code:\n",
    "Data Generation: Similar to previous examples, random data for events and losses is generated.\n",
    "VaR Calculations:\n",
    "Empirical Method: Calculates VaR based on the 99.9 percentile of net loss and event counts.\n",
    "Monte Carlo Simulation: Simulates event counts using a Poisson distribution and loss severity using a log-normal distribution, then calculates VaR.\n",
    "Merging Results: The results from both methods are combined into a single DataFrame.\n",
    "Total VaR and Percentage Distributions: Computes the total VaR for both losses and counts, along with their percentage distributions.\n",
    "Final Output: The final DataFrame includes all relevant metrics and is saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b667bbe-bada-4198-be87-fb9bb517d3b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reports\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a new Document\u001b[39;00m\n\u001b[0;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m Document()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\docx.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     TAGS \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
     ]
    }
   ],
   "source": [
    "# Reports\n",
    "from docx import Document\n",
    "\n",
    "# Create a new Document\n",
    "doc = Document()\n",
    "\n",
    "# Title\n",
    "doc.add_heading('Value at Risk (VaR) Analysis Report', level=1)\n",
    "\n",
    "# Introduction\n",
    "doc.add_heading('1. Introduction', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"This report provides a comprehensive analysis of Value at Risk (VaR) using multiple methods \"\n",
    "    \"to assess the financial risk associated with various events. The analysis utilizes a generated \"\n",
    "    \"dataset consisting of event types, business lines, and their associated net loss amounts over a \"\n",
    "    \"four-year period. The methods employed include the Empirical Method and Monte Carlo Simulation.\"\n",
    ")\n",
    "\n",
    "# Dataset Description\n",
    "doc.add_heading('2. Dataset Description', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"The dataset contains the following columns:\\n\"\n",
    "    \"- Date: Randomly generated dates over a four-year period.\\n\"\n",
    "    \"- Unique Event ID: Unique identifiers for each event.\\n\"\n",
    "    \"- Event Type: Type of the event (e.g., Fraud, Cyber Attack).\\n\"\n",
    "    \"- Business Line: Different lines of business affected by the events.\\n\"\n",
    "    \"- Event Description: Descriptive text for the events.\\n\"\n",
    "    \"- Net Loss Amount: Randomly generated loss amounts for each event.\\n\"\n",
    ")\n",
    "\n",
    "# Methodology\n",
    "doc.add_heading('3. Methodology', level=2)\n",
    "\n",
    "# Empirical Method\n",
    "doc.add_heading('3.1 Empirical Method', level=3)\n",
    "doc.add_paragraph(\n",
    "    \"The empirical method calculates VaR by determining the 99.9 percentile of the net loss amounts \"\n",
    "    \"for each combination of business line and event type. This method provides a direct statistical \"\n",
    "    \"measure of risk based on historical loss data.\"\n",
    ")\n",
    "\n",
    "# Monte Carlo Simulation\n",
    "doc.add_heading('3.2 Monte Carlo Simulation', level=3)\n",
    "doc.add_paragraph(\n",
    "    \"The Monte Carlo simulation method models event frequency using a Poisson distribution and \"\n",
    "    \"loss severity using a log-normal distribution. By simulating a large number of scenarios, it \"\n",
    "    \"calculates potential losses and provides a more dynamic view of risk.\"\n",
    ")\n",
    "\n",
    "# Results\n",
    "doc.add_heading('4. Results', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"The results of the VaR analysis are displayed in the table below. Each method's results are \"\n",
    "    \"compared across various categories.\"\n",
    ")\n",
    "\n",
    "# Create a sample results table for demonstration\n",
    "doc.add_heading('4.1 Summary of Results', level=3)\n",
    "results_table = doc.add_table(rows=1, cols=11)\n",
    "results_table.style = 'Table Grid'\n",
    "hdr_cells = results_table.rows[0].cells\n",
    "hdr_cells[0].text = 'Year'\n",
    "hdr_cells[1].text = 'Business Line'\n",
    "hdr_cells[2].text = 'Event Type'\n",
    "hdr_cells[3].text = 'VaR (99.9%) for Loss (Empirical)'\n",
    "hdr_cells[4].text = 'VaR (99.9%) for Counts (Empirical)'\n",
    "hdr_cells[5].text = 'VaR Percentage Distribution for Loss (Empirical)'\n",
    "hdr_cells[6].text = 'VaR Percentage Distribution for Counts (Empirical)'\n",
    "hdr_cells[7].text = 'VaR (99.9%) for Loss (Monte Carlo)'\n",
    "hdr_cells[8].text = 'VaR (99.9%) for Counts (Monte Carlo)'\n",
    "hdr_cells[9].text = 'VaR Percentage Distribution for Loss'\n",
    "hdr_cells[10].text = 'VaR Percentage Distribution for Counts'\n",
    "\n",
    "# Sample Data (to be replaced with actual results later)\n",
    "sample_data = [\n",
    "    ['2021', 'Retail', 'Fraud', '15000', '20', '15%', '10%', '18000', '22', '18%', '12%'],\n",
    "    ['2022', 'Corporate Banking', 'Cyber Attack', '20000', '25', '20%', '15%', '23000', '30', '25%', '18%'],\n",
    "]\n",
    "\n",
    "# Adding sample data to the table\n",
    "for row in sample_data:\n",
    "    cells = results_table.add_row().cells\n",
    "    for i, value in enumerate(row):\n",
    "        cells[i].text = str(value)\n",
    "\n",
    "# Conclusion\n",
    "doc.add_heading('5. Conclusion', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"This report presented a thorough analysis of Value at Risk using both the empirical method \"\n",
    "    \"and Monte Carlo simulation. The results provide insights into the risk profiles associated with \"\n",
    "    \"different business lines and event types. The Monte Carlo simulation method offers a more robust \"\n",
    "    \"framework for understanding potential losses under varying scenarios.\"\n",
    ")\n",
    "\n",
    "# Save the document\n",
    "doc_path = 'VaR_Analysis_Report.docx'\n",
    "doc.save(doc_path)\n",
    "\n",
    "doc_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f6858fc-b659-45ef-8543-ae050544931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\himanshu singh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.12.1)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/244.3 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 244.3/244.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac7bd2b8-2409-4220-9b48-987ca4f1d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VaR_Analysis_Report.docx'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Create a new Document\n",
    "doc = Document()\n",
    "\n",
    "# Title\n",
    "doc.add_heading('Value at Risk (VaR) Analysis Report', level=1)\n",
    "\n",
    "# Introduction\n",
    "doc.add_heading('1. Introduction', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"This report provides a comprehensive analysis of Value at Risk (VaR) using multiple methods \"\n",
    "    \"to assess the financial risk associated with various events. The analysis utilizes a generated \"\n",
    "    \"dataset consisting of event types, business lines, and their associated net loss amounts over a \"\n",
    "    \"four-year period. The methods employed include the Empirical Method and Monte Carlo Simulation.\"\n",
    ")\n",
    "\n",
    "# Dataset Description\n",
    "doc.add_heading('2. Dataset Description', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"The dataset contains the following columns:\\n\"\n",
    "    \"- Date: Randomly generated dates over a four-year period.\\n\"\n",
    "    \"- Unique Event ID: Unique identifiers for each event.\\n\"\n",
    "    \"- Event Type: Type of the event (e.g., Fraud, Cyber Attack).\\n\"\n",
    "    \"- Business Line: Different lines of business affected by the events.\\n\"\n",
    "    \"- Event Description: Descriptive text for the events.\\n\"\n",
    "    \"- Net Loss Amount: Randomly generated loss amounts for each event.\\n\"\n",
    ")\n",
    "\n",
    "# Methodology\n",
    "doc.add_heading('3. Methodology', level=2)\n",
    "\n",
    "# Empirical Method\n",
    "doc.add_heading('3.1 Empirical Method', level=3)\n",
    "doc.add_paragraph(\n",
    "    \"The empirical method calculates VaR by determining the 99.9 percentile of the net loss amounts \"\n",
    "    \"for each combination of business line and event type. This method provides a direct statistical \"\n",
    "    \"measure of risk based on historical loss data.\"\n",
    ")\n",
    "\n",
    "# Monte Carlo Simulation\n",
    "doc.add_heading('3.2 Monte Carlo Simulation', level=3)\n",
    "doc.add_paragraph(\n",
    "    \"The Monte Carlo simulation method models event frequency using a Poisson distribution and \"\n",
    "    \"loss severity using a log-normal distribution. By simulating a large number of scenarios, it \"\n",
    "    \"calculates potential losses and provides a more dynamic view of risk.\"\n",
    ")\n",
    "\n",
    "# Results\n",
    "doc.add_heading('4. Results', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"The results of the VaR analysis are displayed in the table below. Each method's results are \"\n",
    "    \"compared across various categories.\"\n",
    ")\n",
    "\n",
    "# Create a sample results table for demonstration\n",
    "doc.add_heading('4.1 Summary of Results', level=3)\n",
    "results_table = doc.add_table(rows=1, cols=11)\n",
    "results_table.style = 'Table Grid'\n",
    "hdr_cells = results_table.rows[0].cells\n",
    "hdr_cells[0].text = 'Year'\n",
    "hdr_cells[1].text = 'Business Line'\n",
    "hdr_cells[2].text = 'Event Type'\n",
    "hdr_cells[3].text = 'VaR (99.9%) for Loss (Empirical)'\n",
    "hdr_cells[4].text = 'VaR (99.9%) for Counts (Empirical)'\n",
    "hdr_cells[5].text = 'VaR Percentage Distribution for Loss (Empirical)'\n",
    "hdr_cells[6].text = 'VaR Percentage Distribution for Counts (Empirical)'\n",
    "hdr_cells[7].text = 'VaR (99.9%) for Loss (Monte Carlo)'\n",
    "hdr_cells[8].text = 'VaR (99.9%) for Counts (Monte Carlo)'\n",
    "hdr_cells[9].text = 'VaR Percentage Distribution for Loss'\n",
    "hdr_cells[10].text = 'VaR Percentage Distribution for Counts'\n",
    "\n",
    "# Sample Data (to be replaced with actual results later)\n",
    "sample_data = [\n",
    "    ['2021', 'Retail', 'Fraud', '15000', '20', '15%', '10%', '18000', '22', '18%', '12%'],\n",
    "    ['2022', 'Corporate Banking', 'Cyber Attack', '20000', '25', '20%', '15%', '23000', '30', '25%', '18%'],\n",
    "]\n",
    "\n",
    "# Adding sample data to the table\n",
    "for row in sample_data:\n",
    "    cells = results_table.add_row().cells\n",
    "    for i, value in enumerate(row):\n",
    "        cells[i].text = str(value)\n",
    "\n",
    "# Conclusion\n",
    "doc.add_heading('5. Conclusion', level=2)\n",
    "doc.add_paragraph(\n",
    "    \"This report presented a thorough analysis of Value at Risk using both the empirical method \"\n",
    "    \"and Monte Carlo simulation. The results provide insights into the risk profiles associated with \"\n",
    "    \"different business lines and event types. The Monte Carlo simulation method offers a more robust \"\n",
    "    \"framework for understanding potential losses under varying scenarios.\"\n",
    ")\n",
    "\n",
    "# Save the document\n",
    "doc_path = 'VaR_Analysis_Report.docx'\n",
    "doc.save(doc_path)\n",
    "\n",
    "doc_path\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5964826b-bd55-43ee-9f65-ff92867dfac3",
   "metadata": {},
   "source": [
    "#Explanation\n",
    "Sure! Let's break down the Monte Carlo simulation method in the provided code step by step:\n",
    "\n",
    "Overview\n",
    "This code defines a function named monte_carlo_simulation, which simulates potential future losses and event counts based on historical data using Monte Carlo methods. It specifically models the event counts using a Poisson distribution and the severity of losses using a log-normal distribution. The ultimate goal is to calculate the Value at Risk (VaR) for both losses and event counts at the 99.9% confidence level.\n",
    "\n",
    "Function Breakdown\n",
    "Function Definition:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def monte_carlo_simulation(group, num_simulations=10000):\n",
    "The function takes two parameters:\n",
    "group: A DataFrame that contains data related to events (e.g., losses and event IDs) for a specific business line and event type.\n",
    "num_simulations: The number of simulation iterations to run (default is 10,000).\n",
    "Event Count Simulation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "event_count_mean = group['Unique Event ID'].count()\n",
    "simulated_counts = np.random.poisson(event_count_mean, num_simulations)\n",
    "This block calculates the mean number of events from the Unique Event ID column in the group DataFrame.\n",
    "It then generates simulated counts of events based on a Poisson distribution, where event_count_mean is used as the mean rate of occurrence. This is done for num_simulations iterations.\n",
    "Severity Amount Simulation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "log_severity = np.log(-group['Net Loss Amount'][group['Net Loss Amount'] < 0].dropna())\n",
    "simulated_losses = np.random.lognormal(mean=log_severity.mean(), sigma=log_severity.std(), size=num_simulations)\n",
    "The code first filters the Net Loss Amount to only include negative values (since we're interested in losses) and drops any missing values.\n",
    "It calculates the natural logarithm of these negative loss amounts (after negating them) to fit a log-normal distribution, which requires the data to be positive.\n",
    "The mean and standard deviation of the log-transformed losses are then calculated.\n",
    "Using these parameters, the function generates simulated_losses using the log-normal distribution for num_simulations iterations.\n",
    "Total Loss Calculation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "total_losses = simulated_counts * simulated_losses\n",
    "This line calculates the total losses for each simulation by multiplying the simulated event counts by the simulated severity amounts (losses). This results in an array of total losses for each simulation.\n",
    "Value at Risk Calculation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "var_loss_mc = np.percentile(total_losses, 99.9)\n",
    "var_counts_mc = np.percentile(simulated_counts, 99.9)\n",
    "The function computes the Value at Risk (VaR) for losses at the 99.9th percentile using the np.percentile function. This means it finds the loss amount that is not exceeded with 99.9% probability.\n",
    "Similarly, it calculates the VaR for event counts at the same percentile.\n",
    "Return Results:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "return pd.Series({\n",
    "    'VaR (99.9%) for Loss (Monte Carlo)': var_loss_mc,\n",
    "    'VaR (99.9%) for Counts (Monte Carlo)': var_counts_mc\n",
    "})\n",
    "Finally, the function returns a Pandas Series containing the calculated VaR values for losses and event counts. Each value is labeled appropriately for easy reference.\n",
    "Summary\n",
    "This Monte Carlo simulation method is a powerful technique to estimate potential risks in terms of both frequency and severity of events. By modeling these factors separately and then combining them, the function provides insights into the worst-case scenarios you might expect with a high level of confidence (99.9% in this case). This is particularly useful for financial risk management, allowing institutions to prepare for extreme but plausible loss scenarios."
   ]
  },
  {
   "cell_type": "raw",
   "id": "36e5de77-7974-4ec6-a92b-81a511a29958",
   "metadata": {},
   "source": [
    "To illustrate the process of calculating the natural logarithm of negative loss amounts and transforming them for use with a log-normal distribution, let‚Äôs walk through a specific example step-by-step.\n",
    "\n",
    "Example Scenario\n",
    "Assume we have the following negative loss amounts from a dataset:\n",
    "\n",
    "Unique Event ID\tNet Loss Amount\n",
    "EID00001\t-5000\n",
    "EID00002\t-7000\n",
    "EID00003\t-2000\n",
    "EID00004\t-3000\n",
    "EID00005\t-8000\n",
    "Step 1: Negate the Loss Amounts\n",
    "Since we are working with losses (negative values), we first need to negate these amounts to convert them to positive values.\n",
    "\n",
    "Unique Event ID\tOriginal Net Loss\tNegated Loss\n",
    "EID00001\t-5000\t5000\n",
    "EID00002\t-7000\t7000\n",
    "EID00003\t-2000\t2000\n",
    "EID00004\t-3000\t3000\n",
    "EID00005\t-8000\t8000\n",
    "Step 2: Calculate the Natural Logarithm\n",
    "Next, we take the natural logarithm (ln) of these positive loss values.\n",
    "\n",
    "Using Python‚Äôs NumPy library, we can compute this easily:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "# Negated loss amounts\n",
    "negated_losses = np.array([5000, 7000, 2000, 3000, 8000])\n",
    "\n",
    "# Calculate the natural logarithm\n",
    "log_severity = np.log(negated_losses)\n",
    "The natural logarithm of the negated losses would yield:\n",
    "\n",
    "Negated Loss\tNatural Logarithm (ln)\n",
    "5000\t8.517193\n",
    "7000\t8.853666\n",
    "2000\t7.600902\n",
    "3000\t8.006368\n",
    "8000\t8.987197\n",
    "Step 3: Summary of Log-Severity\n",
    "The resulting log_severity array would contain the logarithmic values:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Output\n",
    "[8.517193, 8.853666, 7.600902, 8.006368, 8.987197]\n",
    "Step 4: Fit Log-Normal Distribution\n",
    "With these log-severity values, you can then calculate the mean and standard deviation, which are necessary for fitting a log-normal distribution:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "mean_log = log_severity.mean()  # Average of the log values\n",
    "std_log = log_severity.std()     # Standard deviation of the log values\n",
    "Conclusion\n",
    "This process converts negative loss amounts into a form suitable for fitting a log-normal distribution, enabling you to simulate losses accurately. Using the mean and standard deviation of the logarithmic values, you can generate random samples that represent the potential severity of losses in future scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d5b4c-7094-44a2-afe3-6d2304a6eb44",
   "metadata": {},
   "source": [
    "why this total_losses = simulated_counts * simulated_losses\n",
    "\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "The line total_losses = simulated_counts * simulated_losses in the Monte Carlo simulation code combines the results of two independent simulations: the simulated counts of events and the simulated severity of those events. Here‚Äôs a detailed breakdown of why this operation is performed:\n",
    "\n",
    "Components of the Calculation\n",
    "Simulated Counts:\n",
    "\n",
    "The simulated_counts array contains the number of events that are simulated for each of the Monte Carlo iterations, which follows a Poisson distribution.\n",
    "Each entry in this array represents the predicted number of incidents (e.g., fraud cases, system failures) that might occur in a given time period based on the mean number of events from the original data.\n",
    "Simulated Losses:\n",
    "\n",
    "The simulated_losses array contains the estimated loss amount associated with each event, simulated using a log-normal distribution based on the historical loss data.\n",
    "Each entry in this array represents the potential financial impact of a single incident that occurs, assuming that each event has a random severity drawn from the log-normal distribution.\n",
    "Why Multiply Them?\n",
    "Total Loss Calculation:\n",
    "\n",
    "The operation simulated_counts * simulated_losses effectively calculates the total losses incurred for each simulation run.\n",
    "If, for example, one iteration of the simulation results in 3 events occurring (from simulated_counts) and the average severity of those events is $2000 (from simulated_losses), the total loss for that iteration would be:\n",
    "Total¬†Loss\n",
    "=\n",
    "Number¬†of¬†Events\n",
    "√ó\n",
    "Average¬†Loss¬†per¬†Event\n",
    "=\n",
    "3\n",
    "√ó\n",
    "2000\n",
    "=\n",
    "6000\n",
    "Total¬†Loss=Number¬†of¬†Events√óAverage¬†Loss¬†per¬†Event=3√ó2000=6000\n",
    "Iteration Across All Simulations:\n",
    "\n",
    "By performing this multiplication for all iterations (i.e., across the num_simulations), you obtain an array of total losses corresponding to each simulated scenario, which can then be analyzed to calculate metrics like Value at Risk (VaR).\n",
    "Example Calculation\n",
    "Let's illustrate with a simple example:\n",
    "\n",
    "Assume the results of a single Monte Carlo iteration yielded:\n",
    "\n",
    "simulated_counts = 5 (5 events predicted)\n",
    "simulated_losses = [1000, 1500, 2000, 500, 3000] (potential losses per event)\n",
    "In this case, the total_losses would be calculated as:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "total_losses = simulated_counts * simulated_losses\n",
    "If you computed this correctly (considering each event's loss):\n",
    "\n",
    "Total loss would aggregate to:\n",
    "5\n",
    "¬†events\n",
    "√ó\n",
    "(\n",
    "1000\n",
    "+\n",
    "1500\n",
    "+\n",
    "2000\n",
    "+\n",
    "500\n",
    "+\n",
    "3000\n",
    ")\n",
    "=\n",
    "5\n",
    "√ó\n",
    "10000\n",
    "=\n",
    "50000\n",
    "5¬†events√ó(1000+1500+2000+500+3000)=5√ó10000=50000\n",
    "However, if you averaged losses, say:\n",
    "\n",
    "Average loss per event \n",
    "=\n",
    "(\n",
    "1000\n",
    "+\n",
    "1500\n",
    "+\n",
    "2000\n",
    "+\n",
    "500\n",
    "+\n",
    "3000\n",
    ")\n",
    "/\n",
    "5\n",
    "=\n",
    "2000\n",
    "=(1000+1500+2000+500+3000)/5=2000\n",
    "Resulting in total losses = \n",
    "5\n",
    "√ó\n",
    "2000\n",
    "=\n",
    "10000\n",
    "5√ó2000=10000\n",
    "Conclusion\n",
    "The multiplication step is essential for combining the simulated frequency and severity of events, allowing for the assessment of potential total losses resulting from a combination of varying counts and severities. This is crucial for risk management and financial analysis, particularly when calculating metrics like Value at Risk (VaR) based on the worst-case financial scenarios derived from these simulations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f36ba490-c8c2-46d8-bf89-7a2a8ee473cf",
   "metadata": {},
   "source": [
    "Let's break down the provided Python code into its components and explain each part in detail, focusing on its purpose and functionality.\n",
    "\n",
    "Overview\n",
    "This code defines a function calculate_var that calculates the Value at Risk (VaR) for operational losses in a financial context. It simulates losses based on two statistical distributions: Poisson distribution for event counts and Log-Normal distribution for loss severity. The VaR is computed at a specified confidence level (default is 99.9%).\n",
    "\n",
    "Detailed Explanation\n",
    "Function Definition\n",
    "python\n",
    "Copy code\n",
    "def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):\n",
    "Parameters:\n",
    "frequency_lambda: The average rate (Œª) of events occurring, used in the Poisson distribution.\n",
    "severity_shape: Shape parameter for the Log-Normal distribution (fitted from historical loss data).\n",
    "severity_scale: Scale parameter for the Log-Normal distribution (fitted from historical loss data).\n",
    "severity_loc: Location parameter for the Log-Normal distribution (fitted from historical loss data).\n",
    "confidence_level: The confidence level for the VaR calculation (default is 0.999 or 99.9%).\n",
    "n_simulations: The number of simulations to run (default is 10,000).\n",
    "Simulating Event Counts\n",
    "python\n",
    "Copy code\n",
    "simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)\n",
    "This line uses the poisson.rvs function from scipy.stats to generate random variates following a Poisson distribution.\n",
    "mu is set to frequency_lambda, which represents the average number of events (e.g., losses) expected in a given time period.\n",
    "size=n_simulations specifies that we want to generate this many random samples (10,000).\n",
    "Simulating Loss Amounts\n",
    "python\n",
    "Copy code\n",
    "simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)\n",
    "This line simulates loss amounts based on a Log-Normal distribution using the lognorm.rvs function.\n",
    "The parameters s, loc, and scale correspond to the shape, location, and scale of the distribution, respectively.\n",
    "Again, we generate 10,000 random samples.\n",
    "Initializing Results Storage\n",
    "python\n",
    "Copy code\n",
    "var_results = []\n",
    "Here, we initialize an empty list called var_results to store the results of the VaR calculations for each business line and event type.\n",
    "Looping Through Business Lines and Event Types\n",
    "python\n",
    "Copy code\n",
    "for business_line in data['Business Line'].unique():\n",
    "    for event_type in data['Event Type'].unique():\n",
    "This nested loop iterates over all unique combinations of business lines and event types in the dataset.\n",
    "Filtering the Dataset\n",
    "python\n",
    "Copy code\n",
    "subset = data[(data['Business Line'] == business_line) & (data['Event Type'] == event_type)]\n",
    "For each combination, we filter the data DataFrame to get a subset containing only the relevant rows.\n",
    "Calculating Frequency and Severity Parameters\n",
    "python\n",
    "Copy code\n",
    "frequency_lambda = subset['Event Count'].mean()\n",
    "severity_shape, severity_loc, severity_scale = lognorm.fit(subset['Loss Amount'])\n",
    "frequency_lambda is calculated as the mean of the event counts in the filtered subset.\n",
    "The lognorm.fit function fits a Log-Normal distribution to the Loss Amount column of the subset, returning the shape, location, and scale parameters.\n",
    "Calculating VaR\n",
    "python\n",
    "Copy code\n",
    "var_value = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc)\n",
    "This line calls the calculate_var function with the calculated frequency and severity parameters to get the VaR for the current business line and event type.\n",
    "Storing Results\n",
    "python\n",
    "Copy code\n",
    "var_results.append({\n",
    "    'Business Line': business_line,\n",
    "    'Event Type': event_type,\n",
    "    'VaR_99.9%': var_value\n",
    "})\n",
    "The results for each combination are appended to the var_results list as a dictionary containing the business line, event type, and the calculated VaR at the 99.9% confidence level.\n",
    "Converting to DataFrame\n",
    "python\n",
    "Copy code\n",
    "var_results_df = pd.DataFrame(var_results)\n",
    "After all calculations are complete, we convert the list of results into a Pandas DataFrame for better readability and analysis.\n",
    "Displaying Results\n",
    "python\n",
    "Copy code\n",
    "var_results_df\n",
    "Finally, the DataFrame containing the VaR results is returned or printed.\n",
    "Total Loss Calculation and VaR\n",
    "Within the calculate_var function, the following occurs to calculate the total losses and VaR:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "total_losses = simulated_event_counts * simulated_loss_amounts\n",
    "var_value = np.percentile(total_losses, confidence_level * 100)\n",
    "total_losses is computed by multiplying the simulated event counts by the corresponding simulated loss amounts for each simulation.\n",
    "The VaR is then determined by calculating the specified percentile of the total_losses array, indicating the maximum expected loss over a defined period at the specified confidence level.\n",
    "Summary\n",
    "The provided code effectively simulates operational loss data based on statistical distributions and calculates the Value at Risk for various business lines and event types, allowing for risk assessment in a financial or insurance context. This approach is useful for understanding potential financial exposure due to operational events and making informed decisions about risk management strategies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Take example\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Let's walk through an example using the calculate_var function and the entire code structure to illustrate how it works with hypothetical data.\n",
    "\n",
    "Example Setup\n",
    "Imagine we have a dataset that represents operational loss events across different business lines in an insurance company. The dataset contains columns for business lines, event types, event counts, and loss amounts.\n",
    "\n",
    "Sample Data\n",
    "Here‚Äôs a small sample of what the data might look like:\n",
    "\n",
    "Business Line\tEvent Type\tEvent Count\tLoss Amount\n",
    "Insurance\tFraud\t10\t2000\n",
    "Insurance\tClaims\t15\t1500\n",
    "Banking\tFraud\t8\t3000\n",
    "Banking\tOperational\t5\t2500\n",
    "Banking\tClaims\t20\t500\n",
    "Insurance\tOperational\t12\t1800\n",
    "Step-by-Step Execution\n",
    "Step 1: Calculate Frequency and Severity Parameters\n",
    "Business Line: Insurance, Event Type: Fraud\n",
    "\n",
    "Event Count: 10 (mean of counts)\n",
    "Loss Amounts: [2000] (only one value, so we cannot fit Log-Normal accurately, but for this example, let‚Äôs say it fits with shape, loc, scale = (0.1, 0, 2000))\n",
    "Business Line: Insurance, Event Type: Claims\n",
    "\n",
    "Event Count: 15\n",
    "Loss Amounts: [1500] (assume it fits with shape, loc, scale = (0.1, 0, 1500))\n",
    "Business Line: Banking, Event Type: Fraud\n",
    "\n",
    "Event Count: 8\n",
    "Loss Amounts: [3000] (assume it fits with shape, loc, scale = (0.2, 0, 3000))\n",
    "Business Line: Banking, Event Type: Operational\n",
    "\n",
    "Event Count: 5\n",
    "Loss Amounts: [2500] (assume it fits with shape, loc, scale = (0.1, 0, 2500))\n",
    "Business Line: Banking, Event Type: Claims\n",
    "\n",
    "Event Count: 20\n",
    "Loss Amounts: [500] (assume it fits with shape, loc, scale = (0.05, 0, 500))\n",
    "Business Line: Insurance, Event Type: Operational\n",
    "\n",
    "Event Count: 12\n",
    "Loss Amounts: [1800] (assume it fits with shape, loc, scale = (0.1, 0, 1800))\n",
    "Step 2: Execute the VaR Calculation\n",
    "Assuming we have defined the calculate_var function and our dataset, we will now execute the calculation.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import poisson, lognorm\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'Business Line': ['Insurance', 'Insurance', 'Banking', 'Banking', 'Banking', 'Insurance'],\n",
    "    'Event Type': ['Fraud', 'Claims', 'Fraud', 'Operational', 'Claims', 'Operational'],\n",
    "    'Event Count': [10, 15, 8, 5, 20, 12],\n",
    "    'Loss Amount': [2000, 1500, 3000, 2500, 500, 1800]\n",
    "})\n",
    "\n",
    "# Function to calculate VaR\n",
    "def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):\n",
    "    # Simulate event counts based on Poisson\n",
    "    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)\n",
    "    \n",
    "    # Simulate severity amounts based on Log-Normal\n",
    "    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)\n",
    "    \n",
    "    # Calculate total loss for each simulation\n",
    "    total_losses = simulated_event_counts * simulated_loss_amounts\n",
    "    \n",
    "    # Calculate the VaR at the specified confidence level\n",
    "    var_value = np.percentile(total_losses, confidence_level * 100)\n",
    "    return var_value\n",
    "\n",
    "# Store results\n",
    "var_results = []\n",
    "\n",
    "# Loop over each business line and event type\n",
    "for business_line in data['Business Line'].unique():\n",
    "    for event_type in data['Event Type'].unique():\n",
    "        # Filter the dataset for the current combination\n",
    "        subset = data[(data['Business Line'] == business_line) & (data['Event Type'] == event_type)]\n",
    "        \n",
    "        # Calculate frequency and severity parameters for the subset\n",
    "        frequency_lambda = subset['Event Count'].mean()\n",
    "        severity_shape, severity_loc, severity_scale = lognorm.fit(subset['Loss Amount'], floc=0)  # Fitting with floc=0 ensures loc=0\n",
    "        \n",
    "        # Calculate VaR for the subset\n",
    "        var_value = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc)\n",
    "        \n",
    "        # Store the results\n",
    "        var_results.append({\n",
    "            'Business Line': business_line,\n",
    "            'Event Type': event_type,\n",
    "            'VaR_99.9%': var_value\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# Display results\n",
    "print(var_results_df)\n",
    "Output Explanation\n",
    "When you run this code, it will print a DataFrame that summarizes the calculated VaR for each combination of business line and event type. The resulting DataFrame (var_results_df) might look something like this (the actual values will depend on random simulations):\n",
    "\n",
    "Business Line\tEvent Type\tVaR_99.9%\n",
    "Insurance\tFraud\t24500\n",
    "Insurance\tClaims\t21000\n",
    "Banking\tFraud\t32000\n",
    "Banking\tOperational\t21000\n",
    "Banking\tClaims\t15000\n",
    "Insurance\tOperational\t18000\n",
    "Interpretation\n",
    "VaR_99.9%: This value indicates the maximum expected loss over a specified period (e.g., one year) for the given business line and event type, with a confidence level of 99.9%.\n",
    "For instance, if the VaR for \"Banking - Fraud\" is 32,000, this means there is only a 0.1% chance that losses will exceed 32,000 during the period under consideration.\n",
    "Conclusion\n",
    "This example illustrates how to apply the calculate_var function to a dataset of operational risk events, simulate potential losses, and derive the Value at Risk for different combinations of business lines and event types. By using real-world data, you can assess financial risks and inform decision-making processes in risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53461c20-638a-4724-be2e-3360cf4e1698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Business Line   Event Type  VaR_99.9%\n",
      "0     Insurance        Fraud    42000.0\n",
      "1     Insurance       Claims    42001.5\n",
      "2     Insurance  Operational    43200.0\n",
      "3       Banking        Fraud    51003.0\n",
      "4       Banking       Claims    17500.0\n",
      "5       Banking  Operational    32500.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import poisson, lognorm\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'Business Line': ['Insurance', 'Insurance', 'Banking', 'Banking', 'Banking', 'Insurance'],\n",
    "    'Event Type': ['Fraud', 'Claims', 'Fraud', 'Operational', 'Claims', 'Operational'],\n",
    "    'Event Count': [10, 15, 8, 5, 20, 12],\n",
    "    'Loss Amount': [2000, 1500, 3000, 2500, 500, 1800]\n",
    "})\n",
    "\n",
    "# Function to calculate VaR\n",
    "def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):\n",
    "    # Simulate event counts based on Poisson\n",
    "    simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)\n",
    "    \n",
    "    # Simulate severity amounts based on Log-Normal\n",
    "    simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)\n",
    "    \n",
    "    # Calculate total loss for each simulation\n",
    "    total_losses = simulated_event_counts * simulated_loss_amounts\n",
    "    \n",
    "    # Calculate the VaR at the specified confidence level\n",
    "    var_value = np.percentile(total_losses, confidence_level * 100)\n",
    "    return var_value\n",
    "\n",
    "# Store results\n",
    "var_results = []\n",
    "\n",
    "# Loop over each business line and event type\n",
    "for business_line in data['Business Line'].unique():\n",
    "    for event_type in data['Event Type'].unique():\n",
    "        # Filter the dataset for the current combination\n",
    "        subset = data[(data['Business Line'] == business_line) & (data['Event Type'] == event_type)]\n",
    "        \n",
    "        # Calculate frequency and severity parameters for the subset\n",
    "        frequency_lambda = subset['Event Count'].mean()\n",
    "        severity_shape, severity_loc, severity_scale = lognorm.fit(subset['Loss Amount'], floc=0)  # Fitting with floc=0 ensures loc=0\n",
    "        \n",
    "        # Calculate VaR for the subset\n",
    "        var_value = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc)\n",
    "        \n",
    "        # Store the results\n",
    "        var_results.append({\n",
    "            'Business Line': business_line,\n",
    "            'Event Type': event_type,\n",
    "            'VaR_99.9%': var_value\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# Display results\n",
    "print(var_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eeecf3-526f-43be-89fd-7f0f2cb3f9d7",
   "metadata": {},
   "source": [
    "Output Explanation\n",
    "When you run this code, it will print a DataFrame that summarizes the calculated VaR for each combination of business line and event type. The resulting DataFrame (var_results_df) might look something like this (the actual values will depend on random simulations):\n",
    "\n",
    "Business Line\tEvent Type\tVaR_99.9%\n",
    "Insurance\tFraud\t24500\n",
    "Insurance\tClaims\t21000\n",
    "Banking\tFraud\t32000\n",
    "Banking\tOperational\t21000\n",
    "Banking\tClaims\t15000\n",
    "Insurance\tOperational\t18000\n",
    "Interpretation\n",
    "VaR_99.9%: This value indicates the maximum expected loss over a specified period (e.g., one year) for the given business line and event type, with a confidence level of 99.9%.\n",
    "For instance, if the VaR for \"Banking - Fraud\" is 32,000, this means there is only a 0.1% chance that losses will exceed 32,000 during the period under consideration.\n",
    "Conclusion\n",
    "This example illustrates how to apply the calculate_var function to a dataset of operational risk events, simulate potential losses, and derive the Value at Risk for different combinations of business lines and event types. By using real-world data, you can assess financial risks and inform decision-making processes in risk management.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc89542-ca87-4139-a9f0-1de3f38bc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here‚Äôs a complete documentation for the calculate_var function and the surrounding code context. This documentation outlines the purpose, parameters, usage, and provides an example for better understanding.\n",
    "\n",
    "# Value at Risk (VaR) Calculation Documentation\n",
    "# Overview\n",
    "# The calculate_var function simulates operational losses based on event counts and loss amounts, calculating the Value at Risk (VaR) at a specified confidence level. VaR provides a statistical measure of the potential loss in value of an asset or portfolio over a defined period for a given confidence interval.\n",
    "\n",
    "# Purpose\n",
    "# The primary goal of this function is to:\n",
    "\n",
    "# Simulate potential loss scenarios using the Poisson and Log-Normal distributions.\n",
    "# Calculate the maximum expected loss (VaR) over a specified confidence level (e.g., 99.9%) for different business lines and event types.\n",
    "# Function Definition\n",
    "# python\n",
    "# Copy code\n",
    "# def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):\n",
    "# Parameters\n",
    "# frequency_lambda (float): The average rate (Œª) of events occurring, used in the Poisson distribution.\n",
    "\n",
    "# severity_shape (float): Shape parameter for the Log-Normal distribution (fitted from historical loss data).\n",
    "\n",
    "# severity_scale (float): Scale parameter for the Log-Normal distribution (fitted from historical loss data).\n",
    "\n",
    "# severity_loc (float): Location parameter for the Log-Normal distribution (fitted from historical loss data).\n",
    "\n",
    "# confidence_level (float, optional): The confidence level for the VaR calculation. Default is 0.999 (99.9%).\n",
    "\n",
    "# n_simulations (int, optional): The number of simulations to run. Default is 10,000.\n",
    "\n",
    "# Returns\n",
    "# var_value (float): The calculated VaR value at the specified confidence level, representing the maximum expected loss over the specified period.\n",
    "# Implementation Example\n",
    "# Required Libraries\n",
    "# To use the function, ensure the following libraries are imported:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import poisson, lognorm\n",
    "# Sample Dataset\n",
    "# Create a sample dataset representing operational loss events:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# data = pd.DataFrame({\n",
    "#     'Business Line': ['Insurance', 'Insurance', 'Banking', 'Banking', 'Banking', 'Insurance'],\n",
    "#     'Event Type': ['Fraud', 'Claims', 'Fraud', 'Operational', 'Claims', 'Operational'],\n",
    "#     'Event Count': [10, 15, 8, 5, 20, 12],\n",
    "#     'Loss Amount': [2000, 1500, 3000, 2500, 500, 1800]\n",
    "# })\n",
    "# Calculate Frequency and Severity Parameters\n",
    "# Loop through each unique business line and event type, filtering the dataset and calculating parameters:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# var_results = []\n",
    "\n",
    "# for business_line in data['Business Line'].unique():\n",
    "#     for event_type in data['Event Type'].unique():\n",
    "#         # Filter the dataset for the current combination\n",
    "#         subset = data[(data['Business Line'] == business_line) & (data['Event Type'] == event_type)]\n",
    "        \n",
    "#         # Calculate frequency and severity parameters for the subset\n",
    "#         frequency_lambda = subset['Event Count'].mean()\n",
    "#         severity_shape, severity_loc, severity_scale = lognorm.fit(subset['Loss Amount'], floc=0)\n",
    "        \n",
    "#         # Calculate VaR for the subset\n",
    "#         var_value = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc)\n",
    "        \n",
    "#         # Store the results\n",
    "#         var_results.append({\n",
    "#             'Business Line': business_line,\n",
    "#             'Event Type': event_type,\n",
    "#             'VaR_99.9%': var_value\n",
    "#         })\n",
    "# Convert Results to DataFrame\n",
    "# Once all calculations are done, convert the results to a DataFrame for readability:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# # Display results\n",
    "# print(var_results_df)\n",
    "# Complete Example Code\n",
    "# python\n",
    "# Copy code\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import poisson, lognorm\n",
    "\n",
    "# # Sample dataset\n",
    "# data = pd.DataFrame({\n",
    "#     'Business Line': ['Insurance', 'Insurance', 'Banking', 'Banking', 'Banking', 'Insurance'],\n",
    "#     'Event Type': ['Fraud', 'Claims', 'Fraud', 'Operational', 'Claims', 'Operational'],\n",
    "#     'Event Count': [10, 15, 8, 5, 20, 12],\n",
    "#     'Loss Amount': [2000, 1500, 3000, 2500, 500, 1800]\n",
    "# })\n",
    "\n",
    "# # Function to calculate VaR\n",
    "# def calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc, confidence_level=0.999, n_simulations=10000):\n",
    "#     # Simulate event counts based on Poisson\n",
    "#     simulated_event_counts = poisson.rvs(mu=frequency_lambda, size=n_simulations)\n",
    "    \n",
    "#     # Simulate severity amounts based on Log-Normal\n",
    "#     simulated_loss_amounts = lognorm.rvs(s=severity_shape, loc=severity_loc, scale=severity_scale, size=n_simulations)\n",
    "    \n",
    "#     # Calculate total loss for each simulation\n",
    "#     total_losses = simulated_event_counts * simulated_loss_amounts\n",
    "    \n",
    "#     # Calculate the VaR at the specified confidence level\n",
    "#     var_value = np.percentile(total_losses, confidence_level * 100)\n",
    "#     return var_value\n",
    "\n",
    "# # Store results\n",
    "# var_results = []\n",
    "\n",
    "# # Loop over each business line and event type\n",
    "# for business_line in data['Business Line'].unique():\n",
    "#     for event_type in data['Event Type'].unique():\n",
    "#         # Filter the dataset for the current combination\n",
    "#         subset = data[(data['Business Line'] == business_line) & (data['Event Type'] == event_type)]\n",
    "        \n",
    "#         # Calculate frequency and severity parameters for the subset\n",
    "#         frequency_lambda = subset['Event Count'].mean()\n",
    "#         severity_shape, severity_loc, severity_scale = lognorm.fit(subset['Loss Amount'], floc=0)\n",
    "        \n",
    "#         # Calculate VaR for the subset\n",
    "#         var_value = calculate_var(frequency_lambda, severity_shape, severity_scale, severity_loc)\n",
    "        \n",
    "#         # Store the results\n",
    "#         var_results.append({\n",
    "#             'Business Line': business_line,\n",
    "#             'Event Type': event_type,\n",
    "#             'VaR_99.9%': var_value\n",
    "#         })\n",
    "\n",
    "# # Convert results to DataFrame for better readability\n",
    "# var_results_df = pd.DataFrame(var_results)\n",
    "\n",
    "# # Display results\n",
    "# print(var_results_df)\n",
    "# Output Interpretation\n",
    "# The output DataFrame var_results_df will contain the VaR for each combination of business line and event type, allowing for an understanding of potential financial risks associated with different operational events. The VaR_99.9% value indicates the maximum expected loss at a confidence level of 99.9%.\n",
    "\n",
    "# Conclusion\n",
    "# This documentation provides a complete understanding of how to calculate the Value at Risk (VaR) for operational losses using simulated data. The approach utilizes statistical distributions to model potential risks, supporting effective risk management strategies in financial environments.\n",
    "\n",
    "# This documentation can serve as a reference for future implementation or for onboarding new team members who need to understand how the VaR calculations are performed within the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a5c69-2e22-43ec-83f2-ceed1a8d41e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
